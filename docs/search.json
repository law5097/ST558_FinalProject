[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558 Final Project EDA - Lee Worthington",
    "section": "",
    "text": "The dataset used for this analysis is derived from the Behavioral Risk Factor Surveillance System (BRFSS) 2015, a health-related telephone survey conducted annually by the Centers for Disease Control and Prevention (CDC). The BRFSS collects data from over 400,000 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services.\nFor this project Ill focus on the diabetes_binary_health_indicators_BRFSS2015.csv file that contains 253680 survey responses, the main goal is to predict Diabetes_binary which has the below levels. - 0: No diabetes - 1: Prediabetes or diabetes\nThere are 21 potential predictors in the data, since I dont know anything about diabetes I will be using all of the predictors when modelling rather than making assumptions about what may be relevant\n\nHighBP: High blood pressure (N/Y)\nHighChol: High cholesterol (N/Y)\nCholCheck: Cholesterol check within the past five years (N/Y)\nBMI: Body mass index (numeric)\nSmoker: Smoker status (N/Y)\nStroke: History of stroke (N/Y)\nHeartDiseaseorAttack: Coronary heart disease or myocardial infarction (N/Y)\nPhysActivity: Physical activity in the past 30 days (N/Y)\nFruits: Consumption of fruits at least once per day (N/Y)\nVeggies: Consumption of vegetables at least once per day (N/Y)\nHvyAlcoholConsump: Heavy alcohol consumption (N/Y)\nAnyHealthcare: Access to healthcare coverage (N/Y)\nNoDocbcCost: Inability to see a doctor due to cost (N/Y)\nGenHlth: General health status (Excellent/VGood/Good/Fair/Poor)\nMentHlth: Days in the past 30 days when mental health was not good (numeric)\nPhysHlth: Days in the past 30 days when physical health was not good (numeric)\nDiffWalk: Difficulty walking or climbing stairs (N/Y)\nSex: Gender (F/M)\nAge: Age categories (18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+)\nEducation: Education level (No School, Elem, Some HS, HS Grad, Some College, College Grad)\nIncome: Income categories (<$10k, $10-15k, $15-20k, $20-25k, $25-35k, $35-50k, $50-75k, >$75k)\n\n\n\n\nThe main goal of the EDA here is to get a better idea of the data and to spot potential relationships in the data, to hopefully build a model that can accurately predict diabetes based on the available predictors. Since most of the predictors are categorical and there are a large number of them, I will mainly focus on understanding the following in the EDA:\n\nUnderstanding the Distribution: Primarily how the distributions for each predictor vary based on having or not having diabetes, in order to identify potentially relevant predictors\nChecking Data Quality: ID any potential issues with the data, that may require adjustment. That being said my general approach is to not clean or remove data unless im certain its an error, which I cannot be with this data"
  },
  {
    "objectID": "EDA.html#print-summary-results",
    "href": "EDA.html#print-summary-results",
    "title": "ST558 Final Project EDA - Lee Worthington",
    "section": "Print summary results",
    "text": "Print summary results\n\nsummary(df)\n\n Diabetes_binary HighBP     HighChol   CholCheck       BMI        Smoker    \n No :218334      N:144851   N:146089   N:  9470   Min.   :12.00   N:141257  \n Pre: 35346      Y:108829   Y:107591   Y:244210   1st Qu.:24.00   Y:112423  \n Yes:     0                                       Median :27.00             \n                                                  Mean   :28.38             \n                                                  3rd Qu.:31.00             \n                                                  Max.   :98.00             \n                                                                            \n Stroke     HeartDiseaseorAttack PhysActivity Fruits     Veggies   \n N:243388   N:229787             N: 61760     N: 92782   N: 47839  \n Y: 10292   Y: 23893             Y:191920     Y:160898   Y:205841  \n                                                                   \n                                                                   \n                                                                   \n                                                                   \n                                                                   \n HvyAlcoholConsump AnyHealthcare NoDocbcCost  GenHlth         MentHlth     \n N:239424          N: 12417      N:232326    Exc  :45299   Min.   : 0.000  \n Y: 14256          Y:241263      Y: 21354    VGood:89084   1st Qu.: 0.000  \n                                             Good :75646   Median : 0.000  \n                                             Fair :31570   Mean   : 3.185  \n                                             Poor :12081   3rd Qu.: 2.000  \n                                                           Max.   :30.000  \n                                                                           \n    PhysHlth      DiffWalk   Sex             Age               Education     \n Min.   : 0.000   N:211005   F:141974   60-64  :33244   None/Kinder :   174  \n 1st Qu.: 0.000   Y: 42675   M:111706   65-69  :32194   Elem        :  4043  \n Median : 0.000                         55-59  :30832   Some HS     :  9478  \n Mean   : 4.242                         50-54  :26314   HS Grad     : 62750  \n 3rd Qu.: 3.000                         70-74  :23533   Some College: 69910  \n Max.   :30.000                         45-49  :19819   College Grad:107325  \n                                        (Other):87744                        \n     Income     \n >$75k  :90385  \n $50-75k:43219  \n $35-50k:36470  \n $25-35k:25883  \n $20-25k:20135  \n $15-20k:15994  \n (Other):21594  \n\n\n\nBased on the dataset descriptions and the data, these are the variables and types:\n\nDiabetes_binary: Factor (0: No diabetes, 1: Prediabetes or diabetes)\nHighBP: Factor (0: No, 1: Yes)\nHighChol: Factor (0: No, 1: Yes)\nCholCheck: Factor (0: No, 1: Yes)\nBMI: Numeric\nSmoker: Factor (0: No, 1: Yes)\nStroke: Factor (0: No, 1: Yes)\nHeartDiseaseorAttack: Factor (0: No, 1: Yes)\nPhysActivity: Factor (0: No, 1: Yes)\nFruits: Factor (0: No, 1: Yes)\nVeggies: Factor (0: No, 1: Yes)\nHvyAlcoholConsump: Factor (0: No, 1: Yes)\nAnyHealthcare: Factor (0: No, 1: Yes)\nNoDocbcCost: Factor (0: No, 1: Yes)\nGenHlth: Factor with 5 levels (1: Excellent, 2: Very good, 3: Good, 4: Fair, 5: Poor)\nMentHlth: Numeric\nPhysHlth: Numeric\nDiffWalk: Factor (0: No, 1: Yes)\nSex: Factor with 2 levels (0: Female, 1: Male)\nAge: Factor with 13 levels (1: 18-24, 2: 25-29, 3: 30-34, 4: 35-39, 5: 40-44, 6: 45-49, 7: 50-54, 8: 55-59, 9: 60-64, 10: 65-69, 11: 70-74, 12: 75-79, 13: 80 or older)\nEducation: Factor with 6 levels (1: Never attended school or only kindergarten, 2: Grades 1 through 8, 3: Grades 9 through 11, 4: Grade 12 or GED, 5: College 1 year to 3 years, 6: College 4 years or more)\nIncome: Factor with 8 levels (1: Less than $10,000, 2: $10,000 to less than $15,000, 3: $15,000 to less than $20,000, 4: $20,000 to less than $25,000, 5: $25,000 to less than $35,000, 6: $35,000 to less than $50,000, 7: $50,000 to less than $75,000, 8: $75,000 or more)\n\n\n\nSome notes\n\nIn the kraggle notes they say Diabetes_binary should have 3 levels (0 = no diabetes 1 = prediabetes 2 = diabetes), but there are no records with value = 2"
  },
  {
    "objectID": "EDA.html#code-for-plots",
    "href": "EDA.html#code-for-plots",
    "title": "ST558 Final Project EDA - Lee Worthington",
    "section": "Code for plots",
    "text": "Code for plots\n\n# plot pairs\n#GGally::ggpairs(df, columns = c(1, 2, 3, 4)) \n#GGally::ggpairs(df, columns = c(1, 5, 6, 7))\n#GGally::ggpairs(df, columns = c(1, 8, 9, 10))\n#GGally::ggpairs(df, columns = c(1, 11, 12, 13))\n#GGally::ggpairs(df, columns = c(1, 14, 15, 16))\n#GGally::ggpairs(df, columns = c(1, 17, 18, 19))\n# Mosts of these are ahrd to read and not very informative, use custom plots instead\n\n# Define different variable types\nfactor_vars_heatmap <- c(\n  \"HighBP\", \"HighChol\", \"CholCheck\", \"Smoker\", \"Stroke\", \"HeartDiseaseorAttack\",\n  \"PhysActivity\", \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"AnyHealthcare\",\n  \"NoDocbcCost\", \"DiffWalk\", \"Sex\"\n)\nfactor_vars_histogram <- c(\"GenHlth\", \"Age\", \"Education\", \"Income\")\nnumeric_vars <- c(\"BMI\", \"MentHlth\", \"PhysHlth\")\n\n# Function to generate heatmap for confusion matrix of factor variables\ngenerate_heatmap <- function(var) {\n  confusion <- table(df$Diabetes_binary, df[[var]])\n  confusion_melted <- melt(confusion)\n  confusion_melted$percentage <- confusion_melted$value / sum(confusion_melted$value) * 100\n  confusion_melted$label <- paste(comma(confusion_melted$value), sprintf(\"(%.1f%%)\", confusion_melted$percentage))\n  \n  ggplot(confusion_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(alpha = 0.5) +\n    geom_text(aes(label = label), color = \"black\") +\n    scale_fill_gradient(low = \"lightgreen\", high = \"darkgreen\") +\n    labs(title = paste(\"Confusion Matrix for\", var),\n         x = \"Diabetes_binary\",\n         y = var,\n         fill = \"Count\") +\n    theme_minimal() +\n    scale_x_discrete(labels = c(\"0\", \"1\")) +\n    scale_y_discrete(labels = c(\"0\", \"1\"))\n}\n\n# Function to generate histogram for numeric variables with log scale on x-axis\ngenerate_histogram_numeric <- function(var) {\n  ggplot(df, aes_string(x = var, fill = \"Diabetes_binary\")) +\n    geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n    #scale_x_log10() +\n    labs(title = paste(\"Histogram of\", var, \"partitioned by Diabetes_binary (Log Scale)\"), x = var, y = \"Count\") +\n    theme_minimal()\n}\n\n# Function to generate boxplot for numeric variables\ngenerate_boxplot_numeric <- function(var) {\n  ggplot(df, aes(x = Diabetes_binary, y = df[[var]], fill = Diabetes_binary)) +\n    geom_boxplot(alpha = 0.5) +\n    scale_y_log10() +\n    labs(title = paste(\"Boxplot of\", var, \"partitioned by Diabetes_binary\"), x = \"Diabetes_binary\", y = var) +\n    theme_minimal()\n}\n\n# Function to generate bar plot for factor variables with more than 3 levels\ngenerate_histogram_factor <- function(var) {\n  df |>\n    group_by(!!sym(var), Diabetes_binary) |>\n    \n    summarise(count = n(), .groups = \"drop\") |>\n    \n    group_by(!!sym(var)) |>\n    \n    mutate(\n      percentage = count / sum(count) * 100,\n      label = paste0(sprintf(\"%.1f\", percentage), \"%\")\n    ) |>\n    \n    ggplot(aes_string(x = var, y = \"count\", fill = \"Diabetes_binary\")) +\n    geom_bar(position = \"stack\", stat = \"identity\", alpha = 0.5) +\n    geom_text(aes(label = label), position = position_stack(vjust = 0.5), color = \"black\") +\n    labs(title = paste(\"Bar Plot of\", var, \"partitioned by Diabetes_binary\"), x = var, y = \"Count\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}"
  },
  {
    "objectID": "EDA.html#generate-plots",
    "href": "EDA.html#generate-plots",
    "title": "ST558 Final Project EDA - Lee Worthington",
    "section": "Generate plots",
    "text": "Generate plots\n\n# Generate heatmaps for factor variables with up to 3 levels\nfor (var in factor_vars_heatmap) {\n  print(generate_heatmap(var))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Generate histograms for factor variables with more than 3 levels\nfor (var in factor_vars_histogram) {\n  print(generate_histogram_factor(var))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n# Generate histograms and box-plots for numeric variables with log scale on x-axis\nfor (var in numeric_vars) {\n  print(generate_histogram_numeric(var))\n  print(generate_boxplot_numeric(var))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plots above, these are my main conclusions\n\nThe data looks complete and there are no obvious issues that pop out to me\nAlmost all the potential predictors look relevant, based on how either the heatmaps or plots are very lopsided/skewed depending on the factor levels"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558 Final Project Model Fitting - Lee Worthington",
    "section": "",
    "text": "The dataset used for this analysis is derived from the Behavioral Risk Factor Surveillance System (BRFSS) 2015, a health-related telephone survey conducted annually by the Centers for Disease Control and Prevention (CDC). The BRFSS collects data from over 400,000 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services.\nFor this project Ill focus on the diabetes_binary_health_indicators_BRFSS2015.csv file that contains 253680 survey responses, the main goal is to predict Diabetes_binary which has the below levels. - 0: No diabetes - 1: Prediabetes or diabetes\nThere are 21 potential predictors in the data (https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/):\n\nHighBP: High blood pressure (N/Y)\nHighChol: High cholesterol (N/Y)\nCholCheck: Cholesterol check within the past five years (N/Y)\nBMI: Body mass index (numeric)\nSmoker: Smoker status (N/Y)\nStroke: History of stroke (N/Y)\nHeartDiseaseorAttack: Coronary heart disease or myocardial infarction (N/Y)\nPhysActivity: Physical activity in the past 30 days (N/Y)\nFruits: Consumption of fruits at least once per day (N/Y)\nVeggies: Consumption of vegetables at least once per day (N/Y)\nHvyAlcoholConsump: Heavy alcohol consumption (N/Y)\nAnyHealthcare: Access to healthcare coverage (N/Y)\nNoDocbcCost: Inability to see a doctor due to cost (N/Y)\nGenHlth: General health status (Excellent/VGood/Good/Fair/Poor)\nMentHlth: Days in the past 30 days when mental health was not good (numeric)\nPhysHlth: Days in the past 30 days when physical health was not good (numeric)\nDiffWalk: Difficulty walking or climbing stairs (N/Y)\nSex: Gender (F/M)\nAge: Age categories (18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+)\nEducation: Education level (No School, Elem, Some HS, HS Grad, Some College, College Grad)\nIncome: Income categories (<$10k, $10-15k, $15-20k, $20-25k, $25-35k, $35-50k, $50-75k, >$75k)\n\n\n\n\nThe main goal of the EDA here is to get a better idea of the data and to spot potential relationships in the data, to hopefully build a model that can accurately predict diabetes based on the available predictors. Since most of the predictors are categorical and there are a large number of them, I will mainly focus on understanding the following in the EDA:\n\nUnderstanding Data Relationships: Primarily how the distributions for each predictor vary based on having or not having diabetes, in order to identify potentially relevant predictors\nChecking Data Quality: ID any potential issues with the data, that may require adjustment. That being said my general approach is to not clean or remove data unless im certain its an error, which I cannot be with this data."
  },
  {
    "objectID": "Modeling.html#logistic-models",
    "href": "Modeling.html#logistic-models",
    "title": "ST558 Final Project Model Fitting - Lee Worthington",
    "section": "Logistic Models",
    "text": "Logistic Models\n\nModel with all main effects\n\n# https://topepo.github.io/caret/model-training-and-tuning.html\n\n# Full main effect model\nlogistic_model_all_main <- train(\n  Diabetes_binary ~ .,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n# Print training model fit\nlogistic_model_all_main # logloss 0.3170608\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3170608\n\n\n\n\nModel with only the significant main effects\n\n# Only include significant effects\nlogistic_model_sig_main <- train(\n  Diabetes_binary ~ HighBP + HighChol + BMI + HvyAlcoholConsump + GenHlth + Income + CholCheck + \n                    Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + AnyHealthcare + \n                    MentHlth + PhysHlth + DiffWalk + Sex + Age,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n# Print training model fit\nlogistic_model_sig_main # logloss 0.3170654\n\nGeneralized Linear Model \n\n177577 samples\n    17 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (37), scaled (37) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142062, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3170654\n\n\n\n\nModel with only the most significant main effects and adding interactions\n\n# Fit logistic model with what appear to be the most impactful predictors \nlogistic_model_sig_main_interaction <- train(\n  Diabetes_binary ~ (HighBP + HighChol + BMI + HvyAlcoholConsump + GenHlth + Age + Sex)^2,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n# Print training model fit\nlogistic_model_sig_main_interaction # logLoss 0.3178275\n\nGeneralized Linear Model \n\n177577 samples\n     7 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (159), scaled (159) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142062, 142062, 142061 \nResampling results:\n\n  logLoss  \n  0.3178275\n\n\n\n\nFinal model fit (all main effects)\n\n#saveRDS(logistic_model_all_main, file = \"logistic_model_7262024.rds\")\nlogistic_model <- readRDS(\"C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//logistic_model_7262024.rds\")\n\n# Print training model fit\nlogistic_model # logloss 0.3170608\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3170608\n\n\n\nLogistic regression results\nOverall the model with all of the main effects has the best performance in terms of minimizing logloss, the other 2 have similar but slightly worse performance"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "ST558 Final Project Model Fitting - Lee Worthington",
    "section": "Classification Tree",
    "text": "Classification Tree\n\n# Fit classification tree\ntree_model <- train(\n  Diabetes_binary ~ .,\n  data = train_data,\n  method = \"rpart\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"), # not needed here, but going to leave it\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  ),\n  tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001))\n)\n\n\n#saveRDS(tree_model, file = \"tree_model_7262024.rds\")\ntree_model <- readRDS(\"C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//tree_model_7262024.rds\")\n\n# Print training model fit\ntree_model # logloss 0.3565012\n\nCART \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.4375360\n  0.001  0.3565012\n  0.002  0.3570912\n  0.003  0.3571563\n  0.004  0.3852713\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "ST558 Final Project Model Fitting - Lee Worthington",
    "section": "Random Forest",
    "text": "Random Forest\n\n# Ranger RF\n  # https://cran.r-project.org/web/packages/ranger/index.html\\\n  # This is a faster implementation of the standard \"rf\", with more parameter options\n  \n# Fit random forest model\nrandom_forest_model <- train(\n  Diabetes_binary ~ .,\n  data = train_data,\n  method = \"ranger\",  #method = \"rf\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"), # not needed here, but going to leave it\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  ),\n  tuneLength = 3 #tuneGrid = expand.grid(mtry = 1:(ncol(train_data) - 1))\n)\n\n# Print training model fit\n#random_forest_model\n\n\n#saveRDS(random_forest_model, file = \"random_forest_model_7262024.rds\")\nrandom_forest_model <- readRDS(\"C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//random_forest_model_7262024.rds\")\n\n# Print training model fit\nrandom_forest_model # logloss 0.3294950\n\nRandom Forest \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results across tuning parameters:\n\n  mtry  splitrule   logLoss  \n   2    gini        0.3294950\n   2    extratrees  0.3344979\n  23    gini        0.3748467\n  23    extratrees  0.3689851\n  45    gini        0.4106339\n  45    extratrees  0.4227986\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 2, splitrule = gini\n and min.node.size = 1."
  },
  {
    "objectID": "Modeling.html#test-set-comparison",
    "href": "Modeling.html#test-set-comparison",
    "title": "ST558 Final Project Model Fitting - Lee Worthington",
    "section": "Test set comparison",
    "text": "Test set comparison\n\n# Set data to generate performance stats on\nreview_data <- test_data\n\n# Convert Diabetes_binary to numeric for logloss\nreview_data$Diabetes_binary_numeric <- ifelse(review_data$Diabetes_binary == \"Y\", 1, 0)\n\n# Generate probabilities so I can calculate log-loss\nreview_data$Diabetes_logistic_prob <- predict(logistic_model, newdata = review_data, type = \"prob\")[,2]\nreview_data$Diabetes_tree_prob <- predict(tree_model, newdata = review_data, type = \"prob\")[,2]\nreview_data$Diabetes_rf_prob <- predict(random_forest_model, newdata = review_data, type = \"prob\")[,2]\n\n# Generate prediction\nreview_data$Diabetes_logistic <- predict(logistic_model, newdata = review_data)\nreview_data$Diabetes_tree <- predict(tree_model, newdata = review_data)\nreview_data$Diabetes_rf <- predict(random_forest_model, newdata = review_data)\n\n# Summary for comparison\nsummary(review_data$Diabetes_binary)\n\n    N     Y \n65500 10603 \n\n# Print log-loss and confusion matrix for each model\ncat(\"Log-Loss for Logistic Regression:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_logistic_prob), \"\\n\")\n\nLog-Loss for Logistic Regression: 0.3175498 \n\nprint(confusionMatrix(review_data$Diabetes_logistic, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 64166  8955\n         Y  1334  1648\n                                          \n               Accuracy : 0.8648          \n                 95% CI : (0.8624, 0.8672)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 0.0004935       \n                                          \n                  Kappa : 0.1933          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9796          \n            Specificity : 0.1554          \n         Pos Pred Value : 0.8775          \n         Neg Pred Value : 0.5526          \n             Prevalence : 0.8607          \n         Detection Rate : 0.8431          \n   Detection Prevalence : 0.9608          \n      Balanced Accuracy : 0.5675          \n                                          \n       'Positive' Class : N               \n                                          \n\ncat(\"Log-Loss for Classification Tree:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_tree_prob), \"\\n\")\n\nLog-Loss for Classification Tree: 0.3552405 \n\nprint(confusionMatrix(review_data$Diabetes_tree, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 64539  9257\n         Y   961  1346\n                                          \n               Accuracy : 0.8657          \n                 95% CI : (0.8633, 0.8681)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 2.623e-05       \n                                          \n                  Kappa : 0.167           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9853          \n            Specificity : 0.1269          \n         Pos Pred Value : 0.8746          \n         Neg Pred Value : 0.5834          \n             Prevalence : 0.8607          \n         Detection Rate : 0.8480          \n   Detection Prevalence : 0.9697          \n      Balanced Accuracy : 0.5561          \n                                          \n       'Positive' Class : N               \n                                          \n\ncat(\"Log-Loss for Random Forest:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_rf_prob), \"\\n\")\n\nLog-Loss for Random Forest: 0.3287741 \n\nprint(confusionMatrix(review_data$Diabetes_rf, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 65458 10488\n         Y    42   115\n                                          \n               Accuracy : 0.8616          \n                 95% CI : (0.8592, 0.8641)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 0.2241          \n                                          \n                  Kappa : 0.0174          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.99936         \n            Specificity : 0.01085         \n         Pos Pred Value : 0.86190         \n         Neg Pred Value : 0.73248         \n             Prevalence : 0.86068         \n         Detection Rate : 0.86012         \n   Detection Prevalence : 0.99794         \n      Balanced Accuracy : 0.50510         \n                                          \n       'Positive' Class : N               \n                                          \n\n# paranoia check\n#review_data %>%\n # group_by(Diabetes_binary, Diabetes_rf) %>%\n  #summarise(count = n()) %>%\n  #print()\n\n\nModel comparison\n\nOn the test set in terms of logloss and accuracy these are all very similar However due to highly skewed data specificity tends to be terrible\nSince the assignment didn’t mention it I’ve opted not to try addressing the skewed data with resampling and other techniques\nThe logistic regression model has the lowest logloss and also does slightly better than the other models labelling the minority class\nSo in this case I’ll use the logistic model as the final model"
  }
]
[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558 Final Project EDA",
    "section": "",
    "text": "Description - The dataset used for this analysis is derived from the Behavioral Risk Factor Surveillance System (BRFSS) 2015, a health-related telephone survey conducted annually by the Centers for Disease Control and Prevention (CDC). The BRFSS collects data from over 400,000 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services. - For this project Ill focus on the diabetes_binary_health_indicators_BRFSS2015.csv file that contains 253680 survey responses, the main goal is to predict Diabetes_binary which has the levels. 0 No diabetes 1 Prediabetes or diabetes\n\n\nThere are 21 potential predictors in the data:\n\nHighBP: High blood pressure (N/Y)\nHighChol: High cholesterol (N/Y)\nCholCheck: Cholesterol check within the past five years (N/Y)\nBMI: Body mass index (numeric)\nSmoker: Smoker status (N/Y)\nStroke: History of stroke (N/Y)\nHeartDiseaseorAttack: Coronary heart disease or myocardial infarction (N/Y)\nPhysActivity: Physical activity in the past 30 days (N/Y)\nFruits: Consumption of fruits at least once per day (N/Y)\nVeggies: Consumption of vegetables at least once per day (N/Y)\nHvyAlcoholConsump: Heavy alcohol consumption (N/Y)\nAnyHealthcare: Access to healthcare coverage (N/Y)\nNoDocbcCost: Inability to see a doctor due to cost (N/Y)\nGenHlth: General health status (Excellent/VGood/Good/Fair/Poor)\nMentHlth: Days in the past 30 days when mental health was not good (numeric)\nPhysHlth: Days in the past 30 days when physical health was not good (numeric)\nDiffWalk: Difficulty walking or climbing stairs (N/Y)\nSex: Gender (F/M)\nAge: Age categories (18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+)\nEducation: Education level (No School, Elem, Some HS, HS Grad, Some College, College Grad)\nIncome: Income categories (<$10k, $10-15k, $15-20k, $20-25k, $25-35k, $35-50k, $50-75k, >$75k)\n\n\n\nThe main goal of the EDA here is to get a better idea of the data and to spot potential relationships in the data, to hopefully build a model that can accurately predict diabetes based on the available predictors. Since most of the predictors are categorical and there are a large number of them, I will mainly focus on understanding the following in the EDA:\n\nUnderstanding the Data: Primarily how the distributions for each predictor vary based on having or not having diabetes, in order to identify potentially relevant predictors\nChecking Data Quality: ID any potential issues with the data, that may require adjustment. That being said my general approach is to not clean or remove data unless im certain its an error, which I cannot be with this data"
  },
  {
    "objectID": "EDA.html#setup-environment-and-data",
    "href": "EDA.html#setup-environment-and-data",
    "title": "ST558 Final Project EDA",
    "section": "Setup environment and data",
    "text": "Setup environment and data\n\nLoad libraries, set seed\n\n# load libraries\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(reshape2)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(knitr)  # For rendering tables in Quarto\n\n# set seed\nset.seed(1)  \n\n\n\nGet data\n\n# Read in the data https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/\ninput_data <- read_csv(\n  'docker/diabetes_binary_health_indicators_BRFSS2015.csv',\n  show_col_types = FALSE\n)\n\n\n\nSet data types and levels where needed\n\n# Data cleanup\ndf <- input_data |>\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c(\"Exc\", \"VGood\", \"Good\", \"Fair\", \"Poor\")),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"F\", \"M\")),\n    Age = factor(\n      Age, \n      levels = 1:13, \n      labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n    ),\n    Education = factor(\n      Education, \n      levels = 1:6, \n      labels = c(\n        \"None/Kinder\", \"Elem\", \"Some HS\", \n        \"HS Grad\", \"Some College\", \"College Grad\"\n      )\n    ),\n    Income = factor(\n      Income, \n      levels = 1:8, \n      labels = c(\n        \"<$10k\", \"$10-15k\", \"$15-20k\", \"$20-25k\", \n        \"$25-35k\", \"$35-50k\", \"$50-75k\", \">$75k\"\n      )\n    )\n  )"
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "ST558 Final Project EDA",
    "section": "EDA",
    "text": "EDA\n\nPrint summary results\n\nsummary(df)\n\n Diabetes_binary HighBP     HighChol   CholCheck       BMI        Smoker    \n N:218334        N:144851   N:146089   N:  9470   Min.   :12.00   N:141257  \n Y: 35346        Y:108829   Y:107591   Y:244210   1st Qu.:24.00   Y:112423  \n                                                  Median :27.00             \n                                                  Mean   :28.38             \n                                                  3rd Qu.:31.00             \n                                                  Max.   :98.00             \n                                                                            \n Stroke     HeartDiseaseorAttack PhysActivity Fruits     Veggies   \n N:243388   N:229787             N: 61760     N: 92782   N: 47839  \n Y: 10292   Y: 23893             Y:191920     Y:160898   Y:205841  \n                                                                   \n                                                                   \n                                                                   \n                                                                   \n                                                                   \n HvyAlcoholConsump AnyHealthcare NoDocbcCost  GenHlth         MentHlth     \n N:239424          N: 12417      N:232326    Exc  :45299   Min.   : 0.000  \n Y: 14256          Y:241263      Y: 21354    VGood:89084   1st Qu.: 0.000  \n                                             Good :75646   Median : 0.000  \n                                             Fair :31570   Mean   : 3.185  \n                                             Poor :12081   3rd Qu.: 2.000  \n                                                           Max.   :30.000  \n                                                                           \n    PhysHlth      DiffWalk   Sex             Age               Education     \n Min.   : 0.000   N:211005   F:141974   60-64  :33244   None/Kinder :   174  \n 1st Qu.: 0.000   Y: 42675   M:111706   65-69  :32194   Elem        :  4043  \n Median : 0.000                         55-59  :30832   Some HS     :  9478  \n Mean   : 4.242                         50-54  :26314   HS Grad     : 62750  \n 3rd Qu.: 3.000                         70-74  :23533   Some College: 69910  \n Max.   :30.000                         45-49  :19819   College Grad:107325  \n                                        (Other):87744                        \n     Income     \n >$75k  :90385  \n $50-75k:43219  \n $35-50k:36470  \n $25-35k:25883  \n $20-25k:20135  \n $15-20k:15994  \n (Other):21594  \n\nsapply(df, function(x) sum(is.na(x)))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\n\nObservations\n\nThere don’t appear to be any missing values within the data, though in age and income there is an “Other” option\nThe only obvious potential issue I see here is that the max BMI is 98, which seems high but I have no expertise in this area so I’ll leave the values alone\nEverything else looks reasonable\n\n\n\n\nCode for plots\n\n# plot pairs\n#GGally::ggpairs(df, columns = c(1, 2, 3, 4))  # Mosts of these are ahrd to read and not very informative, use custom plots instead\n\n# Define different variable types\nfactor_vars_heatmap <- c(\n  \"HighBP\", \"HighChol\", \"CholCheck\", \"Smoker\", \"Stroke\", \"HeartDiseaseorAttack\",\n  \"PhysActivity\", \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"AnyHealthcare\",\n  \"NoDocbcCost\", \"DiffWalk\", \"Sex\"\n)\nfactor_vars_histogram <- c(\"GenHlth\", \"Age\", \"Education\", \"Income\")\nnumeric_vars <- c(\"BMI\", \"MentHlth\", \"PhysHlth\")\n\n# Function to generate heatmap for confusion matrix of factor variables\ngenerate_heatmap <- function(var) {\n  confusion <- table(df$Diabetes_binary, df[[var]])\n  confusion_melted <- melt(confusion)\n  confusion_melted$percentage <- confusion_melted$value / sum(confusion_melted$value) * 100\n  confusion_melted$label <- paste(comma(confusion_melted$value), sprintf(\"(%.1f%%)\", confusion_melted$percentage))\n  \n  ggplot(confusion_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(alpha = 0.5) +\n    geom_text(aes(label = label), color = \"black\") +\n    scale_fill_gradient(low = \"lightgreen\", high = \"darkgreen\") +\n    labs(title = paste(\"Confusion Matrix for\", var),\n         x = \"Diabetes_binary\",\n         y = var,\n         fill = \"Count\") +\n    theme_minimal() +\n    scale_x_discrete(labels = c(\"0\", \"1\")) +\n    scale_y_discrete(labels = c(\"0\", \"1\"))\n}\n\n# Function to generate histogram for numeric variables with log scale on x-axis\ngenerate_histogram_numeric <- function(var) {\n  ggplot(df, aes_string(x = var, fill = \"Diabetes_binary\")) +\n    geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n    #scale_x_log10() +\n    labs(title = paste(\"Histogram of\", var, \"partitioned by Diabetes_binary (Log Scale)\"), x = var, y = \"Count\") +\n    theme_minimal()\n}\n\n# Function to generate boxplot for numeric variables\ngenerate_boxplot_numeric <- function(var) {\n  ggplot(df, aes(x = Diabetes_binary, y = df[[var]], fill = Diabetes_binary)) +\n    geom_boxplot(alpha = 0.5) +\n    scale_y_log10() +\n    labs(title = paste(\"Boxplot of\", var, \"partitioned by Diabetes_binary\"), x = \"Diabetes_binary\", y = var) +\n    theme_minimal()\n}\n\n# Function to generate bar plot for factor variables with more than 3 levels\ngenerate_histogram_factor <- function(var) {\n  df |>\n    group_by(!!sym(var), Diabetes_binary) |>\n    \n    summarise(count = n(), .groups = \"drop\") |>\n    \n    group_by(!!sym(var)) |>\n    \n    mutate(\n      percentage = count / sum(count) * 100,\n      label = paste0(sprintf(\"%.1f\", percentage), \"%\")\n    ) |>\n    \n    ggplot(aes_string(x = var, y = \"count\", fill = \"Diabetes_binary\")) +\n    geom_bar(position = \"stack\", stat = \"identity\", alpha = 0.5) +\n    geom_text(aes(label = label), position = position_stack(vjust = 0.5), color = \"black\") +\n    labs(title = paste(\"Bar Plot of\", var, \"partitioned by Diabetes_binary\"), x = var, y = \"Count\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\n\n\n\nGenerate plots\n\n# Generate heatmaps for factor variables with up to 3 levels\nfor (var in factor_vars_heatmap) {\n  print(generate_heatmap(var))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Generate histograms for factor variables with more than 3 levels\nfor (var in factor_vars_histogram) {\n  print(generate_histogram_factor(var))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n# Generate histograms and box-plots for numeric variables with log scale on x-axis\nfor (var in numeric_vars) {\n  #print(generate_histogram_numeric(var))\n  print(generate_boxplot_numeric(var))\n}\n\n\n\n\n\n\n\n\n\n\n\nGiven the large number of potential predictors and many of them being categorical running all the pair plots is hard to interpret, so I’ll focus on plotting predictors relationships with the response instead of with each other. Based on the plots above:\n\nIn the boxplots we can see that having poor mental and physical health for prolonged periods of time seems to increase diabetes risk\nHigher BMI also seems to be indicative of higher diabetes risk as the distribution for pre-diabetes generally has larger related BMI values\nLower income also seems to be associated wiht a much higher risk of diabetes as generally the higher the income the lower the proportion of diabetes\nHigher education level seems to be associated with lower diabetes risk based on the proportions\nIncreased age also seems to lead to increased diabetes risk\nHaving high blood pressure and high cholestoral seem to increase diabetes risk\nLow general health also seems to be associated with higher diabetes risk\nOverall there look to be a number of potentially strong predictors, but the most impactful seem to be MentHlth PhysHlth GenHlth BMI Income Education Age HighBP HighChol"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "",
    "text": "In this file using the diabetes dataset described in the EDA I’ll fit a series of models attempting to predict the Diabetes_Binary field using log loss as the evaluation metric, models built here include:\n\n3 different logistic regression models\n1 classification tree\n1 random forest\n\n\n\nLog loss\n\nLog loss is a performance metric for evaluating the predictions of a binary classification model that takes on the following form\nLog Loss = - (1/N) * Σ [ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ]\nWhere y_i is the true label, p_i is the predicted probability, and N is the number of instances.\nSince log loss considers probabilities of correct predictions, unlike something like accuracy, it’s more sensitive to imbalanced binary target variables as incorrect predictions will be more heavily penalized which encourages more balanced classification accuracy from the model"
  },
  {
    "objectID": "Modeling.html#logistic-models",
    "href": "Modeling.html#logistic-models",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "Logistic Models",
    "text": "Logistic Models\n\nModel with all main effects\n\n# https://topepo.github.io/caret/model-training-and-tuning.html\n\n# Full main effect model\nlogistic_model_all_main <- train(\n  Diabetes_binary ~ .,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n# Print training model fit\nlogistic_model_all_main # logloss 0.3170608\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3170608\n\n\n\n\nModel with only the significant main effects\n\n# Only include significant effects\nlogistic_model_sig_main <- train(\n  Diabetes_binary ~ HighBP + HighChol + BMI + HvyAlcoholConsump + GenHlth + Income + CholCheck + \n                    Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + AnyHealthcare + \n                    MentHlth + PhysHlth + DiffWalk + Sex + Age,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n# Print training model fit\nlogistic_model_sig_main # logloss 0.3170654\n\nGeneralized Linear Model \n\n177577 samples\n    17 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (37), scaled (37) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142062, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3170654\n\n\n\n\nModel with only the most significant main effects and adding interactions\n\n# Fit logistic model with what appear to be the most impactful predictors \nlogistic_model_sig_main_interaction <- train(\n  Diabetes_binary ~ (HighBP + HighChol + BMI + HvyAlcoholConsump + GenHlth + Age + Sex)^2,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n# Print training model fit\n#logistic_model_sig_main_interaction # logLoss 0.3178275\n\n\n\nFinal model fit (all main effects)\n\n#saveRDS(logistic_model_all_main, file = \"logistic_model_7262024.rds\")\nlogistic_model <- readRDS(\"C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//logistic_model_7262024.rds\")\n\n# Print training model fit\nlogistic_model # logloss 0.3170608\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3170608\n\n\n\nLogistic regression results\nOverall the model with all of the main effects has the best performance in terms of minimizing logloss, the other 2 have similar but slightly worse performance"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "Classification Tree",
    "text": "Classification Tree\n\nExplanation\n\nClassification trees are a non-parametric supervised method used for classifying instances into classes\nThe model splits the data into subsets based by splitting on predictors in a way that minimizes your target metric at each split, in a greedy manner\nThis splitting continues until a predefined point or some threshold set by the user is met such as a mximum depth or minimum node observations\nIn classification the final prediction is based on the mode in the final node of one of the tree paths\nTrees are helpful when your data potentially has complex interactions and/or is non-linear, they are also generally pretty easy to interpret depending on how complex they are\n\n\n\n# Fit classification tree\ntree_model <- train(\n  Diabetes_binary ~ .,\n  data = train_data,\n  method = \"rpart\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"), # not needed here, but going to leave it\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  ),\n  tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001))\n)\n\n\n# The decision tree takes a while to train, so I've saved it here so I don't need to retrain it every time I change/test something in this code\n#saveRDS(tree_model, file = \"tree_model_7262024.rds\")\ntree_model <- readRDS(\"C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//tree_model_7262024.rds\")\n\n# Print training model fit\ntree_model # logloss 0.3565012\n\nCART \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.4375360\n  0.001  0.3565012\n  0.002  0.3570912\n  0.003  0.3571563\n  0.004  0.3852713\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "Random Forest",
    "text": "Random Forest\n\nExplanation\n\nRandom forest is an ensemble learning method that builds multiple decision trees and takes the mode or mean of the ensemble of trees to make a final prediction\nEach tree in the forest is trained on a bootstrap sample (random sample with replacement of size n) and random subset of predictors\nThe process for fitting each tree is the same as in the basic classification tree outlined above\nThe main advantages of this approach is that it prevents overfitting while decreasing variance in predictions by aggregating the predictions from multiple trees, instead of a single tree\n\n\n\n# Ranger RF\n  # https://cran.r-project.org/web/packages/ranger/index.html\\\n  # This is a faster implementation of the standard \"rf\", with more parameter options\n  \n# Fit random forest model\nrandom_forest_model <- train(\n  Diabetes_binary ~ .,\n  data = train_data,\n  method = \"ranger\",  #method = \"rf\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"), # not needed here, but going to leave it\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  ),\n  tuneLength = 3 #tuneGrid = expand.grid(mtry = 1:(ncol(train_data) - 1))\n)\n\n\n# The random forest takes a while to train, so I've saved it here so I don't need to retrain it every time I change/test something in this code\n#saveRDS(random_forest_model, file = \"random_forest_model_7262024.rds\")\nrandom_forest_model <- readRDS(\"C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//random_forest_model_7262024.rds\")\n\n# Print training model fit\nrandom_forest_model # logloss 0.3294950\n\nRandom Forest \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results across tuning parameters:\n\n  mtry  splitrule   logLoss  \n   2    gini        0.3294950\n   2    extratrees  0.3344979\n  23    gini        0.3748467\n  23    extratrees  0.3689851\n  45    gini        0.4106339\n  45    extratrees  0.4227986\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 2, splitrule = gini\n and min.node.size = 1."
  },
  {
    "objectID": "Modeling.html#test-set-comparison",
    "href": "Modeling.html#test-set-comparison",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "Test set comparison",
    "text": "Test set comparison\n\n# Set data to generate performance stats on\nreview_data <- test_data\n\n# Convert Diabetes_binary to numeric for logloss\nreview_data$Diabetes_binary_numeric <- ifelse(review_data$Diabetes_binary == \"Y\", 1, 0)\n\n# Generate probabilities so I can calculate log-loss\nreview_data$Diabetes_logistic_prob <- predict(logistic_model, newdata = review_data, type = \"prob\")[,2]\nreview_data$Diabetes_tree_prob <- predict(tree_model, newdata = review_data, type = \"prob\")[,2]\nreview_data$Diabetes_rf_prob <- predict(random_forest_model, newdata = review_data, type = \"prob\")[,2]\n\n# Generate prediction\nreview_data$Diabetes_logistic <- predict(logistic_model, newdata = review_data)\nreview_data$Diabetes_tree <- predict(tree_model, newdata = review_data)\nreview_data$Diabetes_rf <- predict(random_forest_model, newdata = review_data)\n\n# Summary for comparison\nsummary(review_data$Diabetes_binary)\n\n    N     Y \n65500 10603 \n\n# Print log-loss and confusion matrix for each model\ncat(\"Log-Loss for Logistic Regression:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_logistic_prob), \"\\n\")\n\nLog-Loss for Logistic Regression: 0.3175498 \n\nprint(confusionMatrix(review_data$Diabetes_logistic, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 64166  8955\n         Y  1334  1648\n                                          \n               Accuracy : 0.8648          \n                 95% CI : (0.8624, 0.8672)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 0.0004935       \n                                          \n                  Kappa : 0.1933          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9796          \n            Specificity : 0.1554          \n         Pos Pred Value : 0.8775          \n         Neg Pred Value : 0.5526          \n             Prevalence : 0.8607          \n         Detection Rate : 0.8431          \n   Detection Prevalence : 0.9608          \n      Balanced Accuracy : 0.5675          \n                                          \n       'Positive' Class : N               \n                                          \n\ncat(\"Log-Loss for Classification Tree:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_tree_prob), \"\\n\")\n\nLog-Loss for Classification Tree: 0.3552405 \n\nprint(confusionMatrix(review_data$Diabetes_tree, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 64539  9257\n         Y   961  1346\n                                          \n               Accuracy : 0.8657          \n                 95% CI : (0.8633, 0.8681)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 2.623e-05       \n                                          \n                  Kappa : 0.167           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9853          \n            Specificity : 0.1269          \n         Pos Pred Value : 0.8746          \n         Neg Pred Value : 0.5834          \n             Prevalence : 0.8607          \n         Detection Rate : 0.8480          \n   Detection Prevalence : 0.9697          \n      Balanced Accuracy : 0.5561          \n                                          \n       'Positive' Class : N               \n                                          \n\ncat(\"Log-Loss for Random Forest:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_rf_prob), \"\\n\")\n\nLog-Loss for Random Forest: 0.3287741 \n\nprint(confusionMatrix(review_data$Diabetes_rf, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 65458 10488\n         Y    42   115\n                                          \n               Accuracy : 0.8616          \n                 95% CI : (0.8592, 0.8641)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 0.2241          \n                                          \n                  Kappa : 0.0174          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.99936         \n            Specificity : 0.01085         \n         Pos Pred Value : 0.86190         \n         Neg Pred Value : 0.73248         \n             Prevalence : 0.86068         \n         Detection Rate : 0.86012         \n   Detection Prevalence : 0.99794         \n      Balanced Accuracy : 0.50510         \n                                          \n       'Positive' Class : N               \n                                          \n\n# paranoia check\n#review_data %>%\n # group_by(Diabetes_binary, Diabetes_rf) %>%\n  #summarise(count = n()) %>%\n  #print()\n\n\nModel comparison\n\nOn the test set in terms of logloss and accuracy these are all very similar However due to highly skewed data specificity tends to be terrible\nSince the assignment didn’t mention it I’ve opted not to try addressing the skewed data with resampling and other techniques\nThe logistic regression model has the lowest logloss and also does slightly better than the other models labelling the minority class\nSo in this case I’ll use the logistic model as the final model"
  },
  {
    "objectID": "Modeling.html#setup-environment-and-read-data",
    "href": "Modeling.html#setup-environment-and-read-data",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "Setup environment and read data",
    "text": "Setup environment and read data\n\nLoad libraries, set seed\n\n# load libraries\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(reshape2)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(knitr)  # For rendering tables in Quarto\nlibrary(ranger)\nlibrary(Metrics) # for logloss\n\n# set seed\nset.seed(1)  \n\n\n\nGet data\n\n# Read in the data https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/\ninput_data <- read_csv(\n  'docker/diabetes_binary_health_indicators_BRFSS2015.csv',\n  show_col_types = FALSE\n)\n\n\n\nSet data types and levels where needed\n\n# Data cleanup\ndf <- input_data |>\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c(\"Exc\", \"VGood\", \"Good\", \"Fair\", \"Poor\")),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"N\", \"Y\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"F\", \"M\")),\n    Age = factor(\n      Age, \n      levels = 1:13, \n      labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n    ),\n    Education = factor(\n      Education, \n      levels = 1:6, \n      labels = c(\n        \"None/Kinder\", \"Elem\", \"Some HS\", \n        \"HS Grad\", \"Some College\", \"College Grad\"\n      )\n    ),\n    Income = factor(\n      Income, \n      levels = 1:8, \n      labels = c(\n        \"<$10k\", \"$10-15k\", \"$15-20k\", \"$20-25k\", \n        \"$25-35k\", \"$35-50k\", \"$50-75k\", \">$75k\"\n      )\n    )\n  )\n\n# Split dummy data and drop original fields\ntrain_index <- createDataPartition(df$Diabetes_binary, p = 0.7, list = FALSE)\ntrain_data <- df[train_index, ]\ntest_data <- df[-train_index, ]\n\n# check results\n#summary(as.factor(input_data$Diabetes_binary)) # good\nsummary(df$Diabetes_binary) # good\n\n     N      Y \n218334  35346 \n\nsummary(train_data$Diabetes_binary) # good\n\n     N      Y \n152834  24743 \n\nsummary(test_data$Diabetes_binary) # good\n\n    N     Y \n65500 10603"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\n\nExplanation\n\nLogistic regression is a statistical method used for modeling a binary response variable that predicts the probability that an instance belongs to one of 2 classes\nThe model generates a proability for a particular class, which is then used to assign a final prediction based base on a 0.5 threshold\nThe model has the following form: Logit(p) = ln(p / (1 - p)) = β_0 + β_1 * x_1 + β_2 * x_2 + … + β_n * x_n\nWhere p is the probability of the positive class, β_0 is the intercept, β_1, β_2, …, β_n are the coefficients, and x_1, x_2, …, x_n are the predictor variables\nThe logit function is used to map the predicted values to a range between 0 and 1\n\n\n\nModel with all main effects\n\n# Full main effect model\nlogistic_model_all_main <- train(\n  Diabetes_binary ~ .,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n\n\n# Print training model fit\nlogistic_model_all_main # logloss 0.3170608\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3170608\n\n\n\n\nModel with only the main effects of predictors that looked impactful from EDA and the full model above\n\n# Only include significant effects\nlogistic_model_sig_main <- train(\n  Diabetes_binary ~ BMI + GenHlth + HighBP + HighChol + Age + Income + PhysHlth + MentHlth,\n  #Diabetes_binary ~ BMI + GenHlth + HighBP + HighChol + Age + Income + PhysHlth + MentHlth + Education,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n\n\n# Print training model fit\nlogistic_model_sig_main # logloss 0.3204551\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (28), scaled (28) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142062, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3204551\n\n\n\n\nModel with the subset of predictors and their interactions\n\n# Fit logistic model with what appear to be the most impactful predictors \nlogistic_model_sig_main_interaction <- train(\n  Diabetes_binary ~ (BMI + GenHlth + HighBP + HighChol + Age + Income + PhysHlth + MentHlth)^2,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"logLoss\", # metric caret uses to compare different models\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV\n    classProbs = TRUE,\n    verboseIter = TRUE # Enable progress messages\n  )\n)\n\n+ Fold1: parameter=none \n- Fold1: parameter=none \n+ Fold2: parameter=none \n- Fold2: parameter=none \n+ Fold3: parameter=none \n- Fold3: parameter=none \n+ Fold4: parameter=none \n- Fold4: parameter=none \n+ Fold5: parameter=none \n- Fold5: parameter=none \nAggregating results\nFitting final model on full training set\n\n\n\n# Print training model fit\nsummary(logistic_model_sig_main_interaction) # logLoss 0.3194338\n\n\nCall:\nNULL\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8084  -0.5583  -0.3059  -0.1314   3.8115  \n\nCoefficients:\n                               Estimate Std. Error  z value Pr(>|z|)    \n(Intercept)                  -2.575e+00  1.589e-02 -162.021  < 2e-16 ***\nBMI                           1.303e-01  1.285e-01    1.014 0.310530    \nGenHlthVGood                  4.241e-01  2.762e-01    1.536 0.124605    \nGenHlthGood                   4.213e-01  2.715e-01    1.551 0.120807    \nGenHlthFair                   7.396e-01  2.031e-01    3.641 0.000272 ***\nGenHlthPoor                   4.494e-01  2.132e-01    2.108 0.035055 *  \nHighBPY                       6.132e-01  1.963e-01    3.124 0.001785 ** \nHighCholY                     5.442e-01  1.873e-01    2.906 0.003664 ** \n`Age25-29`                   -1.410e-01  1.681e-01   -0.839 0.401532    \n`Age30-34`                   -2.185e-01  1.920e-01   -1.138 0.255131    \n`Age35-39`                   -1.991e-01  1.916e-01   -1.039 0.298758    \n`Age40-44`                   -1.795e-01  2.007e-01   -0.895 0.370954    \n`Age45-49`                   -6.692e-02  2.142e-01   -0.312 0.754740    \n`Age50-54`                   -1.595e-01  2.392e-01   -0.667 0.504976    \n`Age55-59`                   -1.067e-01  2.541e-01   -0.420 0.674681    \n`Age60-64`                   -5.400e-02  2.611e-01   -0.207 0.836158    \n`Age65-69`                    2.802e-03  2.573e-01    0.011 0.991309    \n`Age70-74`                    3.833e-02  2.253e-01    0.170 0.864894    \n`Age75-79`                    1.203e-01  1.916e-01    0.628 0.530048    \n`Age80+`                      9.447e-02  1.983e-01    0.476 0.633719    \n`Income$10-15k`              -1.858e-01  2.358e-01   -0.788 0.430883    \n`Income$15-20k`              -2.872e-02  1.507e-01   -0.191 0.848816    \n`Income$20-25k`              -4.887e-02  1.696e-01   -0.288 0.773238    \n`Income$25-35k`              -3.133e-01  2.102e-01   -1.490 0.136183    \n`Income$35-50k`              -1.788e-01  2.137e-01   -0.837 0.402783    \n`Income$50-75k`              -2.202e-01  2.287e-01   -0.963 0.335589    \n`Income>$75k`                -5.594e-01  2.801e-01   -1.997 0.045817 *  \nPhysHlth                      7.222e-01  1.886e-01    3.829 0.000129 ***\nMentHlth                     -1.352e-01  1.572e-01   -0.860 0.389959    \n`BMI:GenHlthVGood`           -1.180e-01  7.629e-02   -1.547 0.121960    \n`BMI:GenHlthGood`            -1.447e-01  7.547e-02   -1.917 0.055218 .  \n`BMI:GenHlthFair`            -1.672e-01  5.986e-02   -2.793 0.005216 ** \n`BMI:GenHlthPoor`            -7.868e-02  4.531e-02   -1.737 0.082463 .  \n`BMI:HighBPY`                -1.384e-02  3.780e-02   -0.366 0.714297    \n`BMI:HighCholY`               1.588e-02  3.390e-02    0.468 0.639429    \n`BMI:Age25-29`                1.736e-01  1.008e-01    1.722 0.084994 .  \n`BMI:Age30-34`                1.527e-01  1.197e-01    1.276 0.201925    \n`BMI:Age35-39`                2.832e-01  1.293e-01    2.190 0.028507 *  \n`BMI:Age40-44`                3.229e-01  1.395e-01    2.314 0.020671 *  \n`BMI:Age45-49`                2.940e-01  1.521e-01    1.933 0.053211 .  \n`BMI:Age50-54`                3.775e-01  1.713e-01    2.205 0.027486 *  \n`BMI:Age55-59`                4.436e-01  1.827e-01    2.428 0.015189 *  \n`BMI:Age60-64`                5.344e-01  1.870e-01    2.858 0.004264 ** \n`BMI:Age65-69`                5.552e-01  1.839e-01    3.019 0.002538 ** \n`BMI:Age70-74`                5.846e-01  1.575e-01    3.711 0.000207 ***\n`BMI:Age75-79`                4.453e-01  1.307e-01    3.408 0.000655 ***\n`BMI:Age80+`                  4.774e-01  1.290e-01    3.701 0.000214 ***\n`BMI:Income$10-15k`          -9.300e-02  3.476e-02   -2.675 0.007473 ** \n`BMI:Income$15-20k`           5.325e-02  3.971e-02    1.341 0.179993    \n`BMI:Income$20-25k`          -6.504e-02  4.235e-02   -1.536 0.124562    \n`BMI:Income$25-35k`          -5.155e-02  4.658e-02   -1.107 0.268431    \n`BMI:Income$35-50k`           9.127e-03  5.235e-02    0.174 0.861586    \n`BMI:Income$50-75k`           4.409e-03  5.561e-02    0.079 0.936801    \n`BMI:Income>$75k`            -3.596e-02  6.637e-02   -0.542 0.587949    \n`BMI:PhysHlth`                1.970e-02  3.455e-02    0.570 0.568442    \n`BMI:MentHlth`                7.604e-02  3.108e-02    2.447 0.014417 *  \n`GenHlthVGood:HighBPY`       -1.202e-01  2.946e-02   -4.081 4.48e-05 ***\n`GenHlthGood:HighBPY`        -2.129e-01  3.115e-02   -6.834 8.28e-12 ***\n`GenHlthFair:HighBPY`        -1.652e-01  2.526e-02   -6.541 6.10e-11 ***\n`GenHlthPoor:HighBPY`        -9.384e-02  2.002e-02   -4.688 2.76e-06 ***\n`GenHlthVGood:HighCholY`     -7.074e-02  2.906e-02   -2.435 0.014912 *  \n`GenHlthGood:HighCholY`      -7.193e-02  2.887e-02   -2.491 0.012728 *  \n`GenHlthFair:HighCholY`      -6.710e-02  2.250e-02   -2.982 0.002867 ** \n`GenHlthPoor:HighCholY`      -3.605e-02  1.774e-02   -2.033 0.042072 *  \n`GenHlthVGood:Age25-29`       2.511e-02  8.316e-02    0.302 0.762701    \n`GenHlthGood:Age25-29`        7.619e-02  6.631e-02    1.149 0.250572    \n`GenHlthFair:Age25-29`        4.476e-02  3.429e-02    1.306 0.191699    \n`GenHlthPoor:Age25-29`        2.668e-02  2.153e-02    1.239 0.215258    \n`GenHlthVGood:Age30-34`       1.177e-01  9.310e-02    1.265 0.205991    \n`GenHlthGood:Age30-34`        1.587e-01  7.862e-02    2.019 0.043459 *  \n`GenHlthFair:Age30-34`        7.933e-02  4.208e-02    1.885 0.059408 .  \n`GenHlthPoor:Age30-34`        5.197e-02  2.746e-02    1.893 0.058381 .  \n`GenHlthVGood:Age35-39`       2.776e-02  8.523e-02    0.326 0.744663    \n`GenHlthGood:Age35-39`        8.643e-02  7.405e-02    1.167 0.243157    \n`GenHlthFair:Age35-39`        3.843e-02  4.168e-02    0.922 0.356508    \n`GenHlthPoor:Age35-39`        3.452e-02  3.477e-02    0.993 0.320825    \n`GenHlthVGood:Age40-44`       4.169e-02  8.855e-02    0.471 0.637751    \n`GenHlthGood:Age40-44`        1.660e-01  7.998e-02    2.076 0.037894 *  \n`GenHlthFair:Age40-44`        5.230e-02  4.733e-02    1.105 0.269148    \n`GenHlthPoor:Age40-44`        4.304e-02  4.196e-02    1.026 0.305040    \n`GenHlthVGood:Age45-49`       4.331e-02  9.259e-02    0.468 0.639977    \n`GenHlthGood:Age45-49`        1.656e-01  8.652e-02    1.914 0.055561 .  \n`GenHlthFair:Age45-49`        5.848e-02  5.464e-02    1.070 0.284525    \n`GenHlthPoor:Age45-49`        4.479e-02  5.384e-02    0.832 0.405426    \n`GenHlthVGood:Age50-54`       5.338e-02  1.029e-01    0.519 0.603802    \n`GenHlthGood:Age50-54`        1.922e-01  9.586e-02    2.006 0.044909 *  \n`GenHlthFair:Age50-54`        6.545e-02  6.401e-02    1.023 0.306527    \n`GenHlthPoor:Age50-54`        5.902e-02  7.410e-02    0.796 0.425778    \n`GenHlthVGood:Age55-59`       5.638e-02  1.086e-01    0.519 0.603743    \n`GenHlthGood:Age55-59`        1.950e-01  1.038e-01    1.879 0.060310 .  \n`GenHlthFair:Age55-59`        8.095e-02  7.326e-02    1.105 0.269143    \n`GenHlthPoor:Age55-59`        7.135e-02  8.078e-02    0.883 0.377072    \n`GenHlthVGood:Age60-64`       4.913e-02  1.103e-01    0.445 0.656126    \n`GenHlthGood:Age60-64`        2.218e-01  1.070e-01    2.073 0.038132 *  \n`GenHlthFair:Age60-64`        8.931e-02  7.671e-02    1.164 0.244318    \n`GenHlthPoor:Age60-64`        7.235e-02  8.549e-02    0.846 0.397389    \n`GenHlthVGood:Age65-69`       2.265e-02  1.087e-01    0.208 0.834964    \n`GenHlthGood:Age65-69`        1.844e-01  1.068e-01    1.726 0.084349 .  \n`GenHlthFair:Age65-69`        4.269e-02  7.544e-02    0.566 0.571517    \n`GenHlthPoor:Age65-69`        4.636e-02  7.672e-02    0.604 0.545699    \n`GenHlthVGood:Age70-74`       1.873e-02  9.220e-02    0.203 0.839063    \n`GenHlthGood:Age70-74`        1.404e-01  9.347e-02    1.502 0.133165    \n`GenHlthFair:Age70-74`        2.135e-02  6.638e-02    0.322 0.747742    \n`GenHlthPoor:Age70-74`        2.314e-02  6.780e-02    0.341 0.732871    \n`GenHlthVGood:Age75-79`      -1.006e-02  7.469e-02   -0.135 0.892812    \n`GenHlthGood:Age75-79`        8.511e-02  8.179e-02    1.041 0.298042    \n`GenHlthFair:Age75-79`       -5.109e-03  5.785e-02   -0.088 0.929626    \n`GenHlthPoor:Age75-79`        9.882e-03  6.062e-02    0.163 0.870513    \n`GenHlthVGood:Age80+`        -1.373e-02  7.530e-02   -0.182 0.855319    \n`GenHlthGood:Age80+`          7.654e-02  8.419e-02    0.909 0.363287    \n`GenHlthFair:Age80+`         -2.215e-02  6.431e-02   -0.344 0.730484    \n`GenHlthPoor:Age80+`         -7.401e-05  6.899e-02   -0.001 0.999144    \n`GenHlthVGood:Income$10-15k` -1.740e-03  2.667e-02   -0.065 0.947973    \n`GenHlthGood:Income$10-15k`  -1.741e-02  3.317e-02   -0.525 0.599715    \n`GenHlthFair:Income$10-15k`  -1.321e-02  3.182e-02   -0.415 0.678010    \n`GenHlthPoor:Income$10-15k`  -9.242e-03  2.548e-02   -0.363 0.716850    \n`GenHlthVGood:Income$15-20k` -5.961e-03  3.243e-02   -0.184 0.854153    \n`GenHlthGood:Income$15-20k`  -8.650e-03  3.805e-02   -0.227 0.820161    \n`GenHlthFair:Income$15-20k`  -1.621e-02  3.258e-02   -0.498 0.618738    \n`GenHlthPoor:Income$15-20k`   2.273e-03  2.363e-02    0.096 0.923391    \n`GenHlthVGood:Income$20-25k`  1.265e-02  3.822e-02    0.331 0.740614    \n`GenHlthGood:Income$20-25k`   2.038e-02  4.135e-02    0.493 0.622099    \n`GenHlthFair:Income$20-25k`   8.076e-03  3.162e-02    0.255 0.798417    \n`GenHlthPoor:Income$20-25k`   1.504e-02  2.189e-02    0.687 0.492065    \n`GenHlthVGood:Income$25-35k`  6.386e-02  4.691e-02    1.361 0.173408    \n`GenHlthGood:Income$25-35k`   8.677e-02  4.623e-02    1.877 0.060533 .  \n`GenHlthFair:Income$25-35k`   6.773e-02  3.214e-02    2.107 0.035110 *  \n`GenHlthPoor:Income$25-35k`   5.457e-02  1.998e-02    2.731 0.006313 ** \n`GenHlthVGood:Income$35-50k`  3.921e-02  5.641e-02    0.695 0.486993    \n`GenHlthGood:Income$35-50k`   7.836e-02  5.152e-02    1.521 0.128270    \n`GenHlthFair:Income$35-50k`   4.701e-02  3.097e-02    1.518 0.128965    \n`GenHlthPoor:Income$35-50k`   2.794e-02  1.892e-02    1.477 0.139713    \n`GenHlthVGood:Income$50-75k`  1.198e-01  6.399e-02    1.872 0.061258 .  \n`GenHlthGood:Income$50-75k`   1.350e-01  5.308e-02    2.543 0.010989 *  \n`GenHlthFair:Income$50-75k`   8.398e-02  2.913e-02    2.882 0.003948 ** \n`GenHlthPoor:Income$50-75k`   5.392e-02  1.672e-02    3.225 0.001261 ** \n`GenHlthVGood:Income>$75k`    1.688e-01  8.883e-02    1.901 0.057367 .  \n`GenHlthGood:Income>$75k`     2.090e-01  6.363e-02    3.285 0.001020 ** \n`GenHlthFair:Income>$75k`     1.132e-01  3.075e-02    3.680 0.000233 ***\n`GenHlthPoor:Income>$75k`     6.599e-02  1.622e-02    4.069 4.71e-05 ***\n`GenHlthVGood:PhysHlth`      -1.057e-01  1.997e-02   -5.293 1.21e-07 ***\n`GenHlthGood:PhysHlth`       -1.697e-01  2.690e-02   -6.309 2.81e-10 ***\n`GenHlthFair:PhysHlth`       -2.563e-01  3.554e-02   -7.212 5.53e-13 ***\n`GenHlthPoor:PhysHlth`       -2.798e-01  3.658e-02   -7.648 2.04e-14 ***\n`GenHlthVGood:MentHlth`      -5.711e-02  2.635e-02   -2.167 0.030250 *  \n`GenHlthGood:MentHlth`       -6.388e-02  3.112e-02   -2.053 0.040074 *  \n`GenHlthFair:MentHlth`       -7.968e-02  3.172e-02   -2.512 0.012013 *  \n`GenHlthPoor:MentHlth`       -5.913e-02  2.934e-02   -2.015 0.043860 *  \n`HighBPY:HighCholY`           2.425e-02  1.542e-02    1.573 0.115819    \n`HighBPY:Age25-29`            1.008e-03  2.661e-02    0.038 0.969780    \n`HighBPY:Age30-34`           -8.596e-03  3.299e-02   -0.261 0.794425    \n`HighBPY:Age35-39`            2.028e-02  3.928e-02    0.516 0.605577    \n`HighBPY:Age40-44`            9.942e-03  4.680e-02    0.212 0.831779    \n`HighBPY:Age45-49`            3.234e-02  5.742e-02    0.563 0.573292    \n`HighBPY:Age50-54`            4.719e-02  7.229e-02    0.653 0.513889    \n`HighBPY:Age55-59`            5.178e-02  8.460e-02    0.612 0.540468    \n`HighBPY:Age60-64`            5.728e-02  9.332e-02    0.614 0.539363    \n`HighBPY:Age65-69`            4.109e-02  9.774e-02    0.420 0.674206    \n`HighBPY:Age70-74`           -1.441e-03  8.818e-02   -0.016 0.986964    \n`HighBPY:Age75-79`           -1.174e-02  7.611e-02   -0.154 0.877364    \n`HighBPY:Age80+`             -3.131e-02  7.857e-02   -0.398 0.690282    \n`HighBPY:Income$10-15k`      -2.004e-02  1.642e-02   -1.221 0.222251    \n`HighBPY:Income$15-20k`      -1.402e-02  1.771e-02   -0.792 0.428374    \n`HighBPY:Income$20-25k`      -1.247e-02  1.878e-02   -0.664 0.506626    \n`HighBPY:Income$25-35k`      -2.459e-02  2.023e-02   -1.216 0.224122    \n`HighBPY:Income$35-50k`      -3.035e-02  2.218e-02   -1.368 0.171259    \n`HighBPY:Income$50-75k`      -4.567e-02  2.260e-02   -2.020 0.043361 *  \n`HighBPY:Income>$75k`        -5.158e-02  2.752e-02   -1.874 0.060868 .  \n`HighBPY:PhysHlth`            4.904e-03  1.557e-02    0.315 0.752777    \n`HighBPY:MentHlth`           -6.058e-03  1.313e-02   -0.462 0.644397    \n`HighCholY:Age25-29`         -3.599e-03  2.608e-02   -0.138 0.890216    \n`HighCholY:Age30-34`         -7.451e-03  3.358e-02   -0.222 0.824388    \n`HighCholY:Age35-39`         -5.628e-03  4.049e-02   -0.139 0.889448    \n`HighCholY:Age40-44`         -1.144e-02  4.820e-02   -0.237 0.812469    \n`HighCholY:Age45-49`         -3.280e-02  5.824e-02   -0.563 0.573315    \n`HighCholY:Age50-54`         -4.001e-02  7.221e-02   -0.554 0.579508    \n`HighCholY:Age55-59`         -6.059e-02  8.312e-02   -0.729 0.465996    \n`HighCholY:Age60-64`         -9.739e-02  9.068e-02   -1.074 0.282808    \n`HighCholY:Age65-69`         -1.242e-01  9.295e-02   -1.337 0.181353    \n`HighCholY:Age70-74`         -1.431e-01  8.183e-02   -1.749 0.080311 .  \n`HighCholY:Age75-79`         -1.141e-01  6.735e-02   -1.695 0.090112 .  \n`HighCholY:Age80+`           -1.298e-01  6.783e-02   -1.913 0.055688 .  \n`HighCholY:Income$10-15k`     5.080e-03  1.388e-02    0.366 0.714462    \n`HighCholY:Income$15-20k`     7.575e-03  1.475e-02    0.513 0.607631    \n`HighCholY:Income$20-25k`    -7.351e-03  1.581e-02   -0.465 0.641981    \n`HighCholY:Income$25-35k`     7.461e-03  1.734e-02    0.430 0.666995    \n`HighCholY:Income$35-50k`    -1.866e-02  1.945e-02   -0.959 0.337338    \n`HighCholY:Income$50-75k`    -3.642e-03  2.074e-02   -0.176 0.860619    \n`HighCholY:Income>$75k`      -2.407e-02  2.648e-02   -0.909 0.363471    \n`HighCholY:PhysHlth`          3.354e-02  1.358e-02    2.470 0.013523 *  \n`HighCholY:MentHlth`          1.912e-02  1.228e-02    1.556 0.119643    \n`Age25-29:Income$10-15k`      3.250e-02  3.880e-02    0.838 0.402288    \n`Age30-34:Income$10-15k`      3.713e-02  4.231e-02    0.878 0.380130    \n`Age35-39:Income$10-15k`      7.924e-02  4.280e-02    1.851 0.064103 .  \n`Age40-44:Income$10-15k`      6.814e-02  4.902e-02    1.390 0.164507    \n`Age45-49:Income$10-15k`      7.504e-02  5.610e-02    1.338 0.181012    \n`Age50-54:Income$10-15k`      1.092e-01  7.019e-02    1.556 0.119640    \n`Age55-59:Income$10-15k`      1.111e-01  8.100e-02    1.372 0.170067    \n`Age60-64:Income$10-15k`      1.001e-01  8.514e-02    1.176 0.239731    \n`Age65-69:Income$10-15k`      1.184e-01  8.749e-02    1.353 0.176114    \n`Age70-74:Income$10-15k`      9.406e-02  7.813e-02    1.204 0.228659    \n`Age75-79:Income$10-15k`      1.001e-01  7.028e-02    1.424 0.154343    \n`Age80+:Income$10-15k`        9.382e-02  7.942e-02    1.181 0.237495    \n`Age25-29:Income$15-20k`     -3.108e-02  3.108e-02   -1.000 0.317253    \n`Age30-34:Income$15-20k`     -2.148e-02  3.245e-02   -0.662 0.507971    \n`Age35-39:Income$15-20k`      8.104e-03  3.126e-02    0.259 0.795445    \n`Age40-44:Income$15-20k`     -1.980e-03  3.170e-02   -0.062 0.950198    \n`Age45-49:Income$15-20k`     -5.933e-03  3.483e-02   -0.170 0.864743    \n`Age50-54:Income$15-20k`      6.162e-03  4.127e-02    0.149 0.881296    \n`Age55-59:Income$15-20k`     -9.220e-03  4.552e-02   -0.203 0.839496    \n`Age60-64:Income$15-20k`     -1.627e-02  5.009e-02   -0.325 0.745254    \n`Age65-69:Income$15-20k`     -9.855e-04  5.043e-02   -0.020 0.984408    \n`Age70-74:Income$15-20k`     -2.459e-02  4.652e-02   -0.529 0.597111    \n`Age75-79:Income$15-20k`     -1.013e-02  4.439e-02   -0.228 0.819497    \n`Age80+:Income$15-20k`       -2.037e-02  5.133e-02   -0.397 0.691495    \n`Age25-29:Income$20-25k`     -7.279e-03  3.590e-02   -0.203 0.839340    \n`Age30-34:Income$20-25k`      5.875e-03  3.519e-02    0.167 0.867399    \n`Age35-39:Income$20-25k`      3.407e-02  3.586e-02    0.950 0.341990    \n`Age40-44:Income$20-25k`      1.403e-03  3.517e-02    0.040 0.968188    \n`Age45-49:Income$20-25k`      1.149e-02  3.966e-02    0.290 0.772055    \n`Age50-54:Income$20-25k`      3.431e-02  4.390e-02    0.782 0.434503    \n`Age55-59:Income$20-25k`      2.955e-02  4.980e-02    0.593 0.552872    \n`Age60-64:Income$20-25k`     -2.317e-04  5.557e-02   -0.004 0.996674    \n`Age65-69:Income$20-25k`      3.273e-02  5.951e-02    0.550 0.582310    \n`Age70-74:Income$20-25k`      2.331e-02  5.649e-02    0.413 0.679867    \n`Age75-79:Income$20-25k`      7.778e-03  5.333e-02    0.146 0.884052    \n`Age80+:Income$20-25k`        1.430e-02  5.889e-02    0.243 0.808095    \n`Age25-29:Income$25-35k`     -3.099e-02  4.712e-02   -0.658 0.510727    \n`Age30-34:Income$25-35k`      3.141e-02  4.651e-02    0.675 0.499511    \n`Age35-39:Income$25-35k`      4.203e-02  4.422e-02    0.951 0.341854    \n`Age40-44:Income$25-35k`      3.406e-02  4.297e-02    0.793 0.427962    \n`Age45-49:Income$25-35k`      4.109e-02  4.735e-02    0.868 0.385553    \n`Age50-54:Income$25-35k`      5.882e-02  5.717e-02    1.029 0.303553    \n`Age55-59:Income$25-35k`      6.404e-02  6.225e-02    1.029 0.303613    \n`Age60-64:Income$25-35k`      4.148e-02  7.160e-02    0.579 0.562350    \n`Age65-69:Income$25-35k`      8.566e-02  7.868e-02    1.089 0.276246    \n`Age70-74:Income$25-35k`      5.261e-02  7.352e-02    0.716 0.474269    \n`Age75-79:Income$25-35k`      6.008e-02  6.583e-02    0.913 0.361408    \n`Age80+:Income$25-35k`        5.454e-02  7.149e-02    0.763 0.445511    \n`Age25-29:Income$35-50k`     -6.560e-02  5.021e-02   -1.306 0.191400    \n`Age30-34:Income$35-50k`     -1.641e-02  5.025e-02   -0.327 0.744028    \n`Age35-39:Income$35-50k`      9.723e-03  4.911e-02    0.198 0.843059    \n`Age40-44:Income$35-50k`     -1.016e-02  4.817e-02   -0.211 0.832911    \n`Age45-49:Income$35-50k`     -5.773e-03  5.222e-02   -0.111 0.911974    \n`Age50-54:Income$35-50k`      2.160e-03  6.205e-02    0.035 0.972231    \n`Age55-59:Income$35-50k`      5.899e-03  6.896e-02    0.086 0.931833    \n`Age60-64:Income$35-50k`     -1.775e-02  7.687e-02   -0.231 0.817377    \n`Age65-69:Income$35-50k`      6.840e-03  8.208e-02    0.083 0.933588    \n`Age70-74:Income$35-50k`     -2.651e-03  7.434e-02   -0.036 0.971555    \n`Age75-79:Income$35-50k`     -9.404e-03  6.153e-02   -0.153 0.878523    \n`Age80+:Income$35-50k`       -4.551e-03  6.223e-02   -0.073 0.941710    \n`Age25-29:Income$50-75k`     -7.934e-02  5.350e-02   -1.483 0.138040    \n`Age30-34:Income$50-75k`     -4.534e-02  5.731e-02   -0.791 0.428835    \n`Age35-39:Income$50-75k`     -2.611e-02  5.903e-02   -0.442 0.658316    \n`Age40-44:Income$50-75k`     -3.619e-02  5.932e-02   -0.610 0.541848    \n`Age45-49:Income$50-75k`     -4.187e-02  6.384e-02   -0.656 0.511897    \n`Age50-54:Income$50-75k`      7.057e-03  7.411e-02    0.095 0.924141    \n`Age55-59:Income$50-75k`     -3.051e-02  8.177e-02   -0.373 0.709065    \n`Age60-64:Income$50-75k`     -3.970e-02  8.710e-02   -0.456 0.648505    \n`Age65-69:Income$50-75k`     -1.486e-02  8.708e-02   -0.171 0.864529    \n`Age70-74:Income$50-75k`     -3.407e-02  7.200e-02   -0.473 0.636055    \n`Age75-79:Income$50-75k`     -2.495e-02  5.563e-02   -0.449 0.653734    \n`Age80+:Income$50-75k`       -1.961e-02  5.340e-02   -0.367 0.713507    \n`Age25-29:Income>$75k`       -8.487e-02  6.380e-02   -1.330 0.183481    \n`Age30-34:Income>$75k`        2.689e-02  7.921e-02    0.339 0.734302    \n`Age35-39:Income>$75k`        8.918e-03  9.245e-02    0.096 0.923148    \n`Age40-44:Income>$75k`        1.153e-02  1.003e-01    0.115 0.908466    \n`Age45-49:Income>$75k`       -2.324e-02  1.070e-01   -0.217 0.828068    \n`Age50-54:Income>$75k`        9.747e-02  1.162e-01    0.839 0.401739    \n`Age55-59:Income>$75k`        6.481e-02  1.205e-01    0.538 0.590567    \n`Age60-64:Income>$75k`        4.973e-02  1.160e-01    0.429 0.668177    \n`Age65-69:Income>$75k`        8.495e-02  1.047e-01    0.811 0.417190    \n`Age70-74:Income>$75k`        5.409e-02  8.153e-02    0.663 0.507019    \n`Age75-79:Income>$75k`        2.200e-02  6.051e-02    0.363 0.716234    \n`Age80+:Income>$75k`          4.341e-02  5.491e-02    0.791 0.429195    \n`Age25-29:PhysHlth`          -4.938e-02  2.567e-02   -1.924 0.054378 .  \n`Age30-34:PhysHlth`          -6.415e-02  3.126e-02   -2.052 0.040131 *  \n`Age35-39:PhysHlth`          -1.013e-01  3.663e-02   -2.765 0.005688 ** \n`Age40-44:PhysHlth`          -8.147e-02  4.187e-02   -1.946 0.051669 .  \n`Age45-49:PhysHlth`          -1.079e-01  5.269e-02   -2.047 0.040643 *  \n`Age50-54:PhysHlth`          -1.378e-01  6.570e-02   -2.097 0.036007 *  \n`Age55-59:PhysHlth`          -1.862e-01  7.432e-02   -2.505 0.012255 *  \n`Age60-64:PhysHlth`          -1.927e-01  7.715e-02   -2.498 0.012483 *  \n`Age65-69:PhysHlth`          -1.787e-01  7.433e-02   -2.404 0.016220 *  \n`Age70-74:PhysHlth`          -1.451e-01  6.416e-02   -2.262 0.023684 *  \n`Age75-79:PhysHlth`          -1.393e-01  5.574e-02   -2.500 0.012435 *  \n`Age80+:PhysHlth`            -1.398e-01  6.132e-02   -2.280 0.022600 *  \n`Age25-29:MentHlth`           1.920e-02  3.273e-02    0.587 0.557442    \n`Age30-34:MentHlth`           2.771e-03  3.855e-02    0.072 0.942688    \n`Age35-39:MentHlth`           2.967e-02  4.102e-02    0.723 0.469437    \n`Age40-44:MentHlth`           1.082e-02  4.382e-02    0.247 0.804877    \n`Age45-49:MentHlth`           2.173e-02  4.991e-02    0.435 0.663281    \n`Age50-54:MentHlth`           2.095e-02  5.759e-02    0.364 0.715968    \n`Age55-59:MentHlth`           3.337e-02  6.062e-02    0.551 0.581957    \n`Age60-64:MentHlth`           2.568e-02  5.857e-02    0.438 0.661085    \n`Age65-69:MentHlth`           3.569e-02  5.078e-02    0.703 0.482204    \n`Age70-74:MentHlth`           2.710e-02  3.955e-02    0.685 0.493263    \n`Age75-79:MentHlth`           3.254e-02  3.219e-02    1.011 0.312081    \n`Age80+:MentHlth`             1.965e-02  3.103e-02    0.633 0.526595    \n`Income$10-15k:PhysHlth`      8.741e-04  1.401e-02    0.062 0.950233    \n`Income$15-20k:PhysHlth`      8.192e-03  1.379e-02    0.594 0.552357    \n`Income$20-25k:PhysHlth`      1.226e-02  1.382e-02    0.887 0.375014    \n`Income$25-35k:PhysHlth`     -2.840e-03  1.364e-02   -0.208 0.835031    \n`Income$35-50k:PhysHlth`      4.965e-03  1.402e-02    0.354 0.723322    \n`Income$50-75k:PhysHlth`     -3.445e-03  1.353e-02   -0.255 0.798961    \n`Income>$75k:PhysHlth`       -1.528e-02  1.511e-02   -1.012 0.311759    \n`Income$10-15k:MentHlth`      9.604e-03  1.121e-02    0.857 0.391654    \n`Income$15-20k:MentHlth`      1.467e-03  1.161e-02    0.126 0.899469    \n`Income$20-25k:MentHlth`      3.951e-03  1.158e-02    0.341 0.733035    \n`Income$25-35k:MentHlth`      9.976e-03  1.144e-02    0.872 0.383383    \n`Income$35-50k:MentHlth`      2.614e-02  1.180e-02    2.215 0.026733 *  \n`Income$50-75k:MentHlth`      1.634e-02  1.224e-02    1.335 0.182018    \n`Income>$75k:MentHlth`        1.634e-02  1.421e-02    1.150 0.250320    \n`PhysHlth:MentHlth`           3.554e-02  1.371e-02    2.592 0.009546 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 112629  on 177263  degrees of freedom\nAIC: 113257\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\nFinal model fit (all main effects)\n\n#saveRDS(logistic_model_all_main, file = \"logistic_model_7262024.rds\")\nlogistic_model <- logistic_model_all_main\n\n# Print training model fit\nlogistic_model\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'N', 'Y' \n\nPre-processing: centered (45), scaled (45) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142063, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3170608\n\n\n\nLogistic regression results\n\nOverall the model with all of the main effects has the best performance in terms of minimizing logloss with a value of logloss 0.3170608, so I’ll use the full model as the final model\nThat being said the model with only a subset of the predictors is very close in terms of performance and slightly simpler"
  },
  {
    "objectID": "Modeling.html#test-data-performance-comparison",
    "href": "Modeling.html#test-data-performance-comparison",
    "title": "ST558 Final Project Modelling - Lee Worthington",
    "section": "Test data performance comparison",
    "text": "Test data performance comparison\n\n# Set data to generate performance stats on\nreview_data <- test_data\n\n# Convert Diabetes_binary to numeric for logloss\nreview_data$Diabetes_binary_numeric <- ifelse(review_data$Diabetes_binary == \"Y\", 1, 0)\n\n# Generate probabilities so I can calculate log-loss\nreview_data$Diabetes_logistic_prob <- predict(logistic_model, newdata = review_data, type = \"prob\")[,2]\nreview_data$Diabetes_tree_prob <- predict(tree_model, newdata = review_data, type = \"prob\")[,2]\nreview_data$Diabetes_rf_prob <- predict(random_forest_model, newdata = review_data, type = \"prob\")[,2]\n\n# Generate prediction\nreview_data$Diabetes_logistic <- predict(logistic_model, newdata = review_data)\nreview_data$Diabetes_tree <- predict(tree_model, newdata = review_data)\nreview_data$Diabetes_rf <- predict(random_forest_model, newdata = review_data)\n\n# Summary for comparison\nsummary(review_data$Diabetes_binary)\n\n    N     Y \n65500 10603 \n\n# Print log-loss and confusion matrix for each model\ncat(\"Log-Loss for Logistic Regression:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_logistic_prob), \"\\n\")\n\nLog-Loss for Logistic Regression: 0.3175498 \n\nprint(confusionMatrix(review_data$Diabetes_logistic, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 64166  8955\n         Y  1334  1648\n                                          \n               Accuracy : 0.8648          \n                 95% CI : (0.8624, 0.8672)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 0.0004935       \n                                          \n                  Kappa : 0.1933          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9796          \n            Specificity : 0.1554          \n         Pos Pred Value : 0.8775          \n         Neg Pred Value : 0.5526          \n             Prevalence : 0.8607          \n         Detection Rate : 0.8431          \n   Detection Prevalence : 0.9608          \n      Balanced Accuracy : 0.5675          \n                                          \n       'Positive' Class : N               \n                                          \n\ncat(\"Log-Loss for Classification Tree:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_tree_prob), \"\\n\")\n\nLog-Loss for Classification Tree: 0.3552405 \n\nprint(confusionMatrix(review_data$Diabetes_tree, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 64539  9257\n         Y   961  1346\n                                          \n               Accuracy : 0.8657          \n                 95% CI : (0.8633, 0.8681)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 2.623e-05       \n                                          \n                  Kappa : 0.167           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9853          \n            Specificity : 0.1269          \n         Pos Pred Value : 0.8746          \n         Neg Pred Value : 0.5834          \n             Prevalence : 0.8607          \n         Detection Rate : 0.8480          \n   Detection Prevalence : 0.9697          \n      Balanced Accuracy : 0.5561          \n                                          \n       'Positive' Class : N               \n                                          \n\ncat(\"Log-Loss for Random Forest:\", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_rf_prob), \"\\n\")\n\nLog-Loss for Random Forest: 0.3287741 \n\nprint(confusionMatrix(review_data$Diabetes_rf, review_data$Diabetes_binary))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     N     Y\n         N 65458 10488\n         Y    42   115\n                                          \n               Accuracy : 0.8616          \n                 95% CI : (0.8592, 0.8641)\n    No Information Rate : 0.8607          \n    P-Value [Acc > NIR] : 0.2241          \n                                          \n                  Kappa : 0.0174          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.99936         \n            Specificity : 0.01085         \n         Pos Pred Value : 0.86190         \n         Neg Pred Value : 0.73248         \n             Prevalence : 0.86068         \n         Detection Rate : 0.86012         \n   Detection Prevalence : 0.99794         \n      Balanced Accuracy : 0.50510         \n                                          \n       'Positive' Class : N               \n                                          \n\n# paranoia check\n#review_data %>%\n # group_by(Diabetes_binary, Diabetes_rf) %>%\n  #summarise(count = n()) %>%\n  #print()\n\n\nModel comparison\n\nOn the test set in terms of logloss and accuracy these are all very similar, though none of them label the minority class particularly well\nOverall the logistic regression model has the lowest logloss and also does slightly better than the other models labelling the minority class\nThe logistic regression model is also the easiest to fit in terms of computational cost\nSo in this case I’ll use the logistic model as the final model"
  }
]
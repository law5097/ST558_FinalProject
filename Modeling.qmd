
---
title: "ST558 Final Project Model Fitting - Lee Worthington"
format: html
editor: visual
---

# Intro

> ### Data Overview
>
> The dataset used for this analysis is derived from the Behavioral Risk Factor Surveillance System (BRFSS) 2015, a health-related telephone survey conducted annually by the Centers for Disease Control and Prevention (CDC). The BRFSS collects data from over 400,000 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services.
>
> For this project Ill focus on the diabetes_binary_health_indicators_BRFSS2015.csv file that contains 253680 survey responses, the main goal is to predict Diabetes_binary which has the below levels. - 0: No diabetes - 1: Prediabetes or diabetes
>
> There are 21 potential predictors in the data (https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/):
>
> -   **HighBP**: High blood pressure (N/Y)
> -   **HighChol**: High cholesterol (N/Y)
> -   **CholCheck**: Cholesterol check within the past five years (N/Y)
> -   **BMI**: Body mass index (numeric)
> -   **Smoker**: Smoker status (N/Y)
> -   **Stroke**: History of stroke (N/Y)
> -   **HeartDiseaseorAttack**: Coronary heart disease or myocardial infarction (N/Y)
> -   **PhysActivity**: Physical activity in the past 30 days (N/Y)
> -   **Fruits**: Consumption of fruits at least once per day (N/Y)
> -   **Veggies**: Consumption of vegetables at least once per day (N/Y)
> -   **HvyAlcoholConsump**: Heavy alcohol consumption (N/Y)
> -   **AnyHealthcare**: Access to healthcare coverage (N/Y)
> -   **NoDocbcCost**: Inability to see a doctor due to cost (N/Y)
> -   **GenHlth**: General health status (Excellent/VGood/Good/Fair/Poor)
> -   **MentHlth**: Days in the past 30 days when mental health was not good (numeric)
> -   **PhysHlth**: Days in the past 30 days when physical health was not good (numeric)
> -   **DiffWalk**: Difficulty walking or climbing stairs (N/Y)
> -   **Sex**: Gender (F/M)
> -   **Age**: Age categories (18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+)
> -   **Education**: Education level (No School, Elem, Some HS, HS Grad, Some College, College Grad)
> -   **Income**: Income categories (\<\$10k, \$10-15k, \$15-20k, \$20-25k, \$25-35k, \$35-50k, \$50-75k, \>\$75k)

> ### Purpose of EDA and Modeling Goal
>
> The main goal of the EDA here is to get a better idea of the data and to spot potential relationships in the data, to hopefully build a model that can accurately predict diabetes based on the available predictors. Since most of the predictors are categorical and there are a large number of them, I will mainly focus on understanding the following in the EDA:
>
> 1.  **Understanding Data Relationships**: Primarily how the distributions for each predictor vary based on having or not having diabetes, in order to identify potentially relevant predictors
> 2.  **Checking Data Quality**: ID any potential issues with the data, that may require adjustment. That being said my general approach is to not clean or remove data unless im certain its an error, which I cannot be with this data.

# Setup environment and read data

```{r}
#| eval: true
#| warning: false

# load libraries
library(tidyverse)
library(GGally)
library(caret)
library(corrplot)
library(reshape2)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(scales)
library(knitr)  # For rendering tables in Quarto
library(ranger)
library(Metrics) # for logloss

# set seed
set.seed(1)  

```

```{r}
#| eval: true
#| warning: false

# Read in the data https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/
input_data <- read_csv(
  'diabetes_binary_health_indicators_BRFSS2015.csv',
  show_col_types = FALSE
)

```

# Data prep

```{r}
#| eval: true
#| warning: false

# Data cleanup
df <- input_data |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c("N", "Y")),
    HighBP = factor(HighBP, levels = c(0, 1), labels = c("N", "Y")),
    HighChol = factor(HighChol, levels = c(0, 1), labels = c("N", "Y")),
    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c("N", "Y")),
    Smoker = factor(Smoker, levels = c(0, 1), labels = c("N", "Y")),
    Stroke = factor(Stroke, levels = c(0, 1), labels = c("N", "Y")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c("N", "Y")),
    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c("N", "Y")),
    Fruits = factor(Fruits, levels = c(0, 1), labels = c("N", "Y")),
    Veggies = factor(Veggies, levels = c(0, 1), labels = c("N", "Y")),
    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c("N", "Y")),
    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c("N", "Y")),
    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c("N", "Y")),
    GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c("Exc", "VGood", "Good", "Fair", "Poor")),
    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c("N", "Y")),
    Sex = factor(Sex, levels = c(0, 1), labels = c("F", "M")),
    Age = factor(
      Age, 
      levels = 1:13, 
      labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80+")
    ),
    Education = factor(
      Education, 
      levels = 1:6, 
      labels = c(
        "None/Kinder", "Elem", "Some HS", 
        "HS Grad", "Some College", "College Grad"
      )
    ),
    Income = factor(
      Income, 
      levels = 1:8, 
      labels = c(
        "<$10k", "$10-15k", "$15-20k", "$20-25k", 
        "$25-35k", "$35-50k", "$50-75k", ">$75k"
      )
    )
  )

# Split dummy data and drop original fields
train_index <- createDataPartition(df$Diabetes_binary, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# check results
#summary(as.factor(input_data$Diabetes_binary)) # good
#summary(df$Diabetes_binary) # good
#summary(train_data$Diabetes_binary) # good
#summary(test_data$Diabetes_binary) # good
#head(df) # good

summary(df)

```

# Fit models

## Logistic Models

### Model with all main effects

```{r}
#| eval: true
#| warning: false

# https://topepo.github.io/caret/model-training-and-tuning.html

# Full main effect model
logistic_model_all_main <- train(
  Diabetes_binary ~ .,
  data = train_data,
  method = "glm",
  family = "binomial",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  )
)

# Print training model fit
logistic_model_all_main # logloss 0.3170608

```

### Model with only the significant main effects

```{r}
#| eval: true
#| warning: false

# Only include significant effects
logistic_model_sig_main <- train(
  Diabetes_binary ~ HighBP + HighChol + BMI + HvyAlcoholConsump + GenHlth + Income + CholCheck + 
                    Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + AnyHealthcare + 
                    MentHlth + PhysHlth + DiffWalk + Sex + Age,
  data = train_data,
  method = "glm",
  family = "binomial",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  )
)

# Print training model fit
logistic_model_sig_main # logloss 0.3170654

```

### Model with only the most significant main effects and adding interactions

```{r}
#| eval: true
#| warning: false

# Fit logistic model with what appear to be the most impactful predictors 
logistic_model_sig_main_interaction <- train(
  Diabetes_binary ~ (HighBP + HighChol + BMI + HvyAlcoholConsump + GenHlth + Age + Sex)^2,
  data = train_data,
  method = "glm",
  family = "binomial",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  )
)

# Print training model fit
logistic_model_sig_main_interaction # logLoss 0.3178275

```

### Final model fit (all main effects)

```{r}
#| eval: true
#| warning: false

#saveRDS(logistic_model_all_main, file = "logistic_model_7262024.rds")
logistic_model <- readRDS("C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//logistic_model_7262024.rds")

# Print training model fit
logistic_model # logloss 0.3170608

```

> ### Logistic regression results
>
> Overall the model with all of the main effects has the best performance in terms of minimizing logloss, the other 2 have similar but slightly worse performance

## Classification Tree

```{r}
#| eval: false
#| warning: false

# Fit classification tree
tree_model <- train(
  Diabetes_binary ~ .,
  data = train_data,
  method = "rpart",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"), # not needed here, but going to leave it
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  ),
  tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001))
)

```

```{r}
#| eval: true
#| warning: false

#saveRDS(tree_model, file = "tree_model_7262024.rds")
tree_model <- readRDS("C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//tree_model_7262024.rds")

# Print training model fit
tree_model # logloss 0.3565012

```

## Random Forest

```{r}
#| eval: false
#| warning: false

# Ranger RF
  # https://cran.r-project.org/web/packages/ranger/index.html\
  # This is a faster implementation of the standard "rf", with more parameter options
  
# Fit random forest model
random_forest_model <- train(
  Diabetes_binary ~ .,
  data = train_data,
  method = "ranger",  #method = "rf",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"), # not needed here, but going to leave it
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  ),
  tuneLength = 3 #tuneGrid = expand.grid(mtry = 1:(ncol(train_data) - 1))
)

# Print training model fit
#random_forest_model

```

```{r}
#| eval: true
#| warning: false

#saveRDS(random_forest_model, file = "random_forest_model_7262024.rds")
random_forest_model <- readRDS("C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//random_forest_model_7262024.rds")

# Print training model fit
random_forest_model # logloss 0.3294950

```

## Test set comparison

```{r}
#| eval: true
#| warning: false

# Set data to generate performance stats on
review_data <- test_data

# Convert Diabetes_binary to numeric for logloss
review_data$Diabetes_binary_numeric <- ifelse(review_data$Diabetes_binary == "Y", 1, 0)

# Generate probabilities so I can calculate log-loss
review_data$Diabetes_logistic_prob <- predict(logistic_model, newdata = review_data, type = "prob")[,2]
review_data$Diabetes_tree_prob <- predict(tree_model, newdata = review_data, type = "prob")[,2]
review_data$Diabetes_rf_prob <- predict(random_forest_model, newdata = review_data, type = "prob")[,2]

# Generate prediction
review_data$Diabetes_logistic <- predict(logistic_model, newdata = review_data)
review_data$Diabetes_tree <- predict(tree_model, newdata = review_data)
review_data$Diabetes_rf <- predict(random_forest_model, newdata = review_data)

# Summary for comparison
summary(review_data$Diabetes_binary)

# Print log-loss and confusion matrix for each model
cat("Log-Loss for Logistic Regression:", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_logistic_prob), "\n")
print(confusionMatrix(review_data$Diabetes_logistic, review_data$Diabetes_binary))

cat("Log-Loss for Classification Tree:", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_tree_prob), "\n")
print(confusionMatrix(review_data$Diabetes_tree, review_data$Diabetes_binary))

cat("Log-Loss for Random Forest:", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_rf_prob), "\n")
print(confusionMatrix(review_data$Diabetes_rf, review_data$Diabetes_binary))

# paranoia check
#review_data %>%
 # group_by(Diabetes_binary, Diabetes_rf) %>%
  #summarise(count = n()) %>%
  #print()

```

> ## Model comparison
>
> - On the test set in terms of logloss and accuracy these are all very similar However due to highly skewed data specificity tends to be terrible 
> - Since the assignment didn't mention it I've opted not to try addressing the skewed data with resampling and other techniques
> - The logistic regression model has the lowest logloss and also does slightly better than the other models labelling the minority class
> - So in this case I'll use the logistic model as the final model



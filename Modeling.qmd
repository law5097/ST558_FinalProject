---
title: "ST558 Final Project Modelling - Lee Worthington"
format: 
  html:
    embed-resources: true
editor: visual
editor_options: 
  chunk_output_type: inline
---

## Intro
> In this file using the diabetes dataset described in the EDA I'll fit a series of models attempting to predict the Diabetes_Binary field using log loss as the evaluation metric, models built here include:
>
> - 3 different logistic regression models
> - 1 classification tree
> - 1 random forest

> Log loss
>
> - Log loss is a performance metric for evaluating the predictions of a binary classification model that takes on the following form
> - Log Loss = - (1/N) * Σ [ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ]
> - Where y_i is the true label, p_i is the predicted probability, and N is the number of instances.
> - Since log loss considers probabilities of correct predictions, unlike something like accuracy, it's more sensitive to imbalanced binary target variables as incorrect predictions will be more heavily penalized which encourages more balanced classification accuracy from the model

## Setup environment and read data
### Load libraries, set seed
```{r}
#| eval: true
#| warning: false
#| message: false

# load libraries
library(tidyverse)
library(GGally)
library(caret)
library(corrplot)
library(reshape2)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(scales)
library(knitr)  # For rendering tables in Quarto
library(ranger)
library(Metrics) # for logloss

# set seed
set.seed(1)  

```

### Get data
```{r}
#| eval: true
#| warning: false

# Read in the data https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/
input_data <- read_csv(
  'docker/diabetes_binary_health_indicators_BRFSS2015.csv',
  show_col_types = FALSE
)

```

### Set data types and levels where needed
```{r}
#| eval: true
#| warning: false

# Data cleanup
df <- input_data |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c("N", "Y")),
    HighBP = factor(HighBP, levels = c(0, 1), labels = c("N", "Y")),
    HighChol = factor(HighChol, levels = c(0, 1), labels = c("N", "Y")),
    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c("N", "Y")),
    Smoker = factor(Smoker, levels = c(0, 1), labels = c("N", "Y")),
    Stroke = factor(Stroke, levels = c(0, 1), labels = c("N", "Y")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c("N", "Y")),
    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c("N", "Y")),
    Fruits = factor(Fruits, levels = c(0, 1), labels = c("N", "Y")),
    Veggies = factor(Veggies, levels = c(0, 1), labels = c("N", "Y")),
    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c("N", "Y")),
    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c("N", "Y")),
    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c("N", "Y")),
    GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c("Exc", "VGood", "Good", "Fair", "Poor")),
    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c("N", "Y")),
    Sex = factor(Sex, levels = c(0, 1), labels = c("F", "M")),
    Age = factor(
      Age, 
      levels = 1:13, 
      labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80+")
    ),
    Education = factor(
      Education, 
      levels = 1:6, 
      labels = c(
        "None/Kinder", "Elem", "Some HS", 
        "HS Grad", "Some College", "College Grad"
      )
    ),
    Income = factor(
      Income, 
      levels = 1:8, 
      labels = c(
        "<$10k", "$10-15k", "$15-20k", "$20-25k", 
        "$25-35k", "$35-50k", "$50-75k", ">$75k"
      )
    )
  )

# Split dummy data and drop original fields
train_index <- createDataPartition(df$Diabetes_binary, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# check results
#summary(as.factor(input_data$Diabetes_binary)) # good
summary(df$Diabetes_binary) # good
summary(train_data$Diabetes_binary) # good
summary(test_data$Diabetes_binary) # good

```

## Logistic Regression Models
> Explanation
>
> - Logistic regression is a statistical method used for modeling a binary response variable that predicts the probability that an instance belongs to one of 2 classes
> - The model generates a proability for a particular class, which is then used to assign a final prediction based base on a 0.5 threshold
> - The model has the following form: Logit(p) = ln(p / (1 - p)) = β_0 + β_1 * x_1 + β_2 * x_2 + ... + β_n * x_n
> - Where p is the probability of the positive class, β_0 is the intercept, β_1, β_2, ..., β_n are the coefficients, and x_1, x_2, ..., x_n are the predictor variables
> - The logit function is used to map the predicted values to a range between 0 and 1

### Model with all main effects
```{r}
#| eval: true
#| warning: false

# Full main effect model
logistic_model_all_main <- train(
  Diabetes_binary ~ .,
  data = train_data,
  method = "glm",
  family = "binomial",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  )
)
```

```{r}
#| eval: true
#| warning: false

# Print training model fit
logistic_model_all_main # logloss 0.3170608
```

### Model with only the main effects of predictors that looked impactful from EDA and the full model above
```{r}
#| eval: true
#| warning: false

# Only include significant effects
logistic_model_sig_main <- train(
  Diabetes_binary ~ BMI + GenHlth + HighBP + HighChol + Age + Income + PhysHlth + MentHlth,
  #Diabetes_binary ~ BMI + GenHlth + HighBP + HighChol + Age + Income + PhysHlth + MentHlth + Education,
  data = train_data,
  method = "glm",
  family = "binomial",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  )
)

```

```{r}
#| eval: true
#| warning: false

# Print training model fit
logistic_model_sig_main # logloss 0.3204551
```

### Model with the subset of predictors and their interactions
```{r}
#| eval: true
#| warning: false

# Fit logistic model with what appear to be the most impactful predictors 
logistic_model_sig_main_interaction <- train(
  Diabetes_binary ~ (BMI + GenHlth + HighBP + HighChol + Age + Income + PhysHlth + MentHlth)^2,
  data = train_data,
  method = "glm",
  family = "binomial",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  )
)

```

```{r}
#| eval: true
#| warning: false

# Print training model fit
summary(logistic_model_sig_main_interaction) # logLoss 0.3194338
```

### Final model fit (all main effects)
```{r}
#| eval: true
#| warning: false

#saveRDS(logistic_model_all_main, file = "logistic_model_7262024.rds")
logistic_model <- logistic_model_all_main

# Print training model fit
logistic_model
```
> Logistic regression results
>
> - Overall the model with all of the main effects has the best performance in terms of minimizing logloss with a value of logloss 0.3170608, so I'll use the full model as the final model
> - That being said the model with only a subset of the predictors is very close in terms of performance and slightly simpler 

## Classification Tree
> Explanation
>
> - Classification trees are a non-parametric supervised method used for classifying instances into classes
> - The model splits the data into subsets based by splitting on predictors in a way that minimizes your target metric at each split, in a greedy manner
> - This splitting continues until a predefined point or some threshold set by the user is met such as a mximum depth or minimum node observations
> - In classification the final prediction is based on the mode in the final node of one of the tree paths
> - Trees are helpful when your data potentially has complex interactions and/or is non-linear, they are also generally pretty easy to interpret depending on how complex they are

```{r}
#| eval: false
#| warning: false

# Fit classification tree
tree_model <- train(
  Diabetes_binary ~ .,
  data = train_data,
  method = "rpart",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"), # not needed here, but going to leave it
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  ),
  tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001))
)
```

```{r}
#| eval: true
#| warning: false

# The decision tree takes a while to train, so I've saved it here so I don't need to retrain it every time I change/test something in this code
#saveRDS(tree_model, file = "tree_model_7262024.rds")
tree_model <- readRDS("C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//tree_model_7262024.rds")

# Print training model fit
tree_model # logloss 0.3565012
```

## Random Forest
> Explanation
>
> - Random forest is an ensemble learning method that builds multiple decision trees and takes the mode or mean of the ensemble of trees to make a final prediction
> - Each tree in the forest is trained on a bootstrap sample (random sample with replacement of size n) and random subset of predictors
> - The process for fitting each tree is the same as in the basic classification tree outlined above
> - The main advantages of this approach is that it prevents overfitting while decreasing variance in predictions by aggregating the predictions from multiple trees, instead of a single tree

```{r}
#| eval: false
#| warning: false

# Ranger RF
  # https://cran.r-project.org/web/packages/ranger/index.html\
  # This is a faster implementation of the standard "rf", with more parameter options
  
# Fit random forest model
random_forest_model <- train(
  Diabetes_binary ~ .,
  data = train_data,
  method = "ranger",  #method = "rf",
  metric = "logLoss", # metric caret uses to compare different models
  preProcess = c("center", "scale"), # not needed here, but going to leave it
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    summaryFunction = mnLogLoss, # metric to evaluate a single model during CV
    classProbs = TRUE,
    verboseIter = TRUE # Enable progress messages
  ),
  tuneLength = 3 #tuneGrid = expand.grid(mtry = 1:(ncol(train_data) - 1))
)
```

```{r}
#| eval: true
#| warning: false

# The random forest takes a while to train, so I've saved it here so I don't need to retrain it every time I change/test something in this code
#saveRDS(random_forest_model, file = "random_forest_model_7262024.rds")
random_forest_model <- readRDS("C://Users//lawor//OneDrive//Desktop//School//ST 558//Projects//random_forest_model_7262024.rds")

# Print training model fit
random_forest_model # logloss 0.3294950
```

## Test data performance comparison
```{r}
#| eval: true
#| warning: false

# Set data to generate performance stats on
review_data <- test_data

# Convert Diabetes_binary to numeric for logloss
review_data$Diabetes_binary_numeric <- ifelse(review_data$Diabetes_binary == "Y", 1, 0)

# Generate probabilities so I can calculate log-loss
review_data$Diabetes_logistic_prob <- predict(logistic_model, newdata = review_data, type = "prob")[,2]
review_data$Diabetes_tree_prob <- predict(tree_model, newdata = review_data, type = "prob")[,2]
review_data$Diabetes_rf_prob <- predict(random_forest_model, newdata = review_data, type = "prob")[,2]

# Generate prediction
review_data$Diabetes_logistic <- predict(logistic_model, newdata = review_data)
review_data$Diabetes_tree <- predict(tree_model, newdata = review_data)
review_data$Diabetes_rf <- predict(random_forest_model, newdata = review_data)

# Summary for comparison
summary(review_data$Diabetes_binary)

# Print log-loss and confusion matrix for each model
cat("Log-Loss for Logistic Regression:", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_logistic_prob), "\n")
print(confusionMatrix(review_data$Diabetes_logistic, review_data$Diabetes_binary))

cat("Log-Loss for Classification Tree:", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_tree_prob), "\n")
print(confusionMatrix(review_data$Diabetes_tree, review_data$Diabetes_binary))

cat("Log-Loss for Random Forest:", logLoss(review_data$Diabetes_binary_numeric, review_data$Diabetes_rf_prob), "\n")
print(confusionMatrix(review_data$Diabetes_rf, review_data$Diabetes_binary))

# paranoia check
#review_data %>%
 # group_by(Diabetes_binary, Diabetes_rf) %>%
  #summarise(count = n()) %>%
  #print()

```

> Model comparison
>
> - On the test set in terms of logloss and accuracy these are all very similar, though none of them label the minority class particularly well
> - Overall the logistic regression model has the lowest logloss and also does slightly better than the other models labelling the minority class
> - The logistic regression model is also the easiest to fit in terms of computational cost
> - So in this case I'll use the logistic model as the final model
